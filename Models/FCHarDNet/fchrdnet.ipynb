{"cells":[{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:07:57.390921Z","iopub.status.busy":"2020-10-20T15:07:57.390115Z","iopub.status.idle":"2020-10-20T15:07:58.656831Z","shell.execute_reply":"2020-10-20T15:07:58.656072Z"},"papermill":{"duration":1.319304,"end_time":"2020-10-20T15:07:58.656968","exception":false,"start_time":"2020-10-20T15:07:57.337664","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\n#import network\n#import utils\nimport os\nimport random\nimport argparse\nimport numpy as np\n\nfrom torch.utils import data\nfrom time import perf_counter\n#from datasets import VOCSegmentation, Cityscapes\n#from utils import ext_transforms as et\n#from metrics import StreamSegMetricsd\nimport time\nimport torch\nimport torch.nn as nn\n#from utils.visualizer import Visualizer\nfrom threading import Thread\nimport IPython\n\nfrom PIL import Image\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport pickle\nimport zipfile","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.043386,"end_time":"2020-10-20T15:07:58.744827","exception":false,"start_time":"2020-10-20T15:07:58.701441","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport collections\n\nclass ConvLayer(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel=3, stride=1, dropout=0.1):\n        super().__init__()\n        self.add_module('conv', nn.Conv2d(in_channels, out_channels, kernel_size=kernel,\n                                          stride=stride, padding=kernel//2, bias = False))\n        self.add_module('norm', nn.BatchNorm2d(out_channels))\n        self.add_module('relu', nn.ReLU(inplace=True))\n\n        #print(kernel, 'x', kernel, 'x', in_channels, 'x', out_channels)\n\n    def forward(self, x):\n        return super().forward(x)\n        \n\nclass BRLayer(nn.Sequential):\n    def __init__(self, in_channels):\n        super().__init__()\n        \n        self.add_module('norm', nn.BatchNorm2d(in_channels))\n        self.add_module('relu', nn.ReLU(True))\n    def forward(self, x):\n        return super().forward(x)\n\n\nclass HarDBlock_v2(nn.Module):\n    def get_link(self, layer, base_ch, growth_rate, grmul):\n        if layer == 0:\n          return base_ch, 0, []\n        out_channels = growth_rate\n        link = []\n        for i in range(10):\n          dv = 2 ** i\n          if layer % dv == 0:\n            k = layer - dv\n            link.insert(0, k)\n            if i > 0:\n                out_channels *= grmul\n        out_channels = int(int(out_channels + 1) / 2) * 2\n        in_channels = 0\n        for i in link:\n          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n          in_channels += ch\n        return out_channels, in_channels, link\n\n    def get_out_ch(self):\n        return self.out_channels\n\n    def __init__(self, in_channels, growth_rate, grmul, n_layers, dwconv=False):\n        super().__init__()\n        self.links = []\n        conv_layers_ = []\n        bnrelu_layers_ = []\n        self.layer_bias = []\n        self.out_channels = 0\n        self.out_partition = collections.defaultdict(list)\n\n        for i in range(n_layers):\n          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n          self.links.append(link)\n          for j in link:\n            self.out_partition[j].append(outch)\n\n        cur_ch = in_channels\n        for i in range(n_layers):\n          accum_out_ch = sum( self.out_partition[i] )\n          real_out_ch = self.out_partition[i][0]\n          #print( self.links[i],  self.out_partition[i], accum_out_ch)\n          conv_layers_.append( nn.Conv2d(cur_ch, accum_out_ch, kernel_size=3, stride=1, padding=1, bias=True) )\n          bnrelu_layers_.append( BRLayer(real_out_ch) )\n          cur_ch = real_out_ch\n          if (i % 2 == 0) or (i == n_layers - 1):\n            self.out_channels += real_out_ch\n        #print(\"Blk out =\",self.out_channels)\n\n        self.conv_layers = nn.ModuleList(conv_layers_)\n        self.bnrelu_layers = nn.ModuleList(bnrelu_layers_)\n    \n    def transform(self, blk, trt=False):\n        # Transform weight matrix from a pretrained HarDBlock v1\n        in_ch = blk.layers[0][0].weight.shape[1]\n        for i in range(len(self.conv_layers)):\n            link = self.links[i].copy()\n            link_ch = [blk.layers[k-1][0].weight.shape[0] if k > 0 else \n                       blk.layers[0  ][0].weight.shape[1] for k in link]\n            part = self.out_partition[i]\n            w_src = blk.layers[i][0].weight\n            b_src = blk.layers[i][0].bias\n            \n            \n            self.conv_layers[i].weight[0:part[0], :, :,:] = w_src[:, 0:in_ch, :,:]\n            self.layer_bias.append(b_src)\n            \n            if b_src is not None:\n                if trt:\n                    self.conv_layers[i].bias[1:part[0]] = b_src[1:]\n                    self.conv_layers[i].bias[0] = b_src[0]\n                    self.conv_layers[i].bias[part[0]:] = 0\n                    self.layer_bias[i] = None\n                else:\n                    #for pytorch, add bias with standalone tensor is more efficient than within conv.bias\n                    #this is because the amount of non-zero bias is small, \n                    #but if we use conv.bias, the number of bias will be much larger\n                    self.conv_layers[i].bias = None\n            else:\n                self.conv_layers[i].bias = None \n\n            in_ch = part[0]\n            link_ch.reverse()\n            link.reverse()\n            if len(link) > 1:\n                for j in range(1, len(link) ):\n                    ly  = link[j]\n                    part_id  = self.out_partition[ly].index(part[0])\n                    chos = sum( self.out_partition[ly][0:part_id] )\n                    choe = chos + part[0]\n                    chis = sum( link_ch[0:j] )\n                    chie = chis + link_ch[j]\n                    self.conv_layers[ly].weight[chos:choe, :,:,:] = w_src[:, chis:chie,:,:]\n            \n            #update BatchNorm or remove it if there is no BatchNorm in the v1 block\n            self.bnrelu_layers[i] = None\n            if isinstance(blk.layers[i][1], nn.BatchNorm2d):\n                self.bnrelu_layers[i] = nn.Sequential(\n                         blk.layers[i][1],\n                         blk.layers[i][2])\n            else:\n                self.bnrelu_layers[i] = blk.layers[i][1]\n                    \n\n    def forward(self, x):\n        layers_ = []\n        outs_ = []\n        xin = x\n        for i in range(len(self.conv_layers)):\n            link = self.links[i]\n            part = self.out_partition[i]\n\n            xout = self.conv_layers[i](xin)\n            layers_.append(xout)\n\n            xin = xout[:,0:part[0],:,:] if len(part) > 1 else xout\n            if self.layer_bias[i] is not None:\n                xin += self.layer_bias[i].view(1,-1,1,1)\n\n            if len(link) > 1:\n                for j in range( len(link) - 1 ):\n                    ly  = link[j]\n                    part_id  = self.out_partition[ly].index(part[0])\n                    chs = sum( self.out_partition[ly][0:part_id] )\n                    che = chs + part[0]                    \n                    \n                    xin += layers_[ly][:,chs:che,:,:]\n                    \n            xin = self.bnrelu_layers[i](xin)\n\n            if i%2 == 0 or i == len(self.conv_layers)-1:\n              outs_.append(xin)\n\n        out = torch.cat(outs_, 1)\n        return out\n\n\nclass HarDBlock(nn.Module):\n    def get_link(self, layer, base_ch, growth_rate, grmul):\n        if layer == 0:\n          return base_ch, 0, []\n        out_channels = growth_rate\n        link = []\n        for i in range(10):\n          dv = 2 ** i\n          if layer % dv == 0:\n            k = layer - dv\n            link.append(k)\n            if i > 0:\n                out_channels *= grmul\n        out_channels = int(int(out_channels + 1) / 2) * 2\n        in_channels = 0\n        for i in link:\n          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)\n          in_channels += ch\n        return out_channels, in_channels, link\n\n    def get_out_ch(self):\n        return self.out_channels\n \n    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.growth_rate = growth_rate\n        self.grmul = grmul\n        self.n_layers = n_layers\n        self.keepBase = keepBase\n        self.links = []\n        layers_ = []\n        self.out_channels = 0 # if upsample else in_channels\n        for i in range(n_layers):\n          outch, inch, link = self.get_link(i+1, in_channels, growth_rate, grmul)\n          self.links.append(link)\n          use_relu = residual_out\n          layers_.append(ConvLayer(inch, outch))\n          if (i % 2 == 0) or (i == n_layers - 1):\n            self.out_channels += outch\n        #print(\"Blk out =\",self.out_channels)\n        self.layers = nn.ModuleList(layers_)\n\n\n    def forward(self, x):\n        layers_ = [x]\n        for layer in range(len(self.layers)):\n            link = self.links[layer]\n            tin = []\n            for i in link:\n                tin.append(layers_[i])\n            if len(tin) > 1:\n                x = torch.cat(tin, 1)\n            else:\n                x = tin[0]\n            out = self.layers[layer](x)\n            layers_.append(out)\n        t = len(layers_)\n        out_ = []\n        for i in range(t):\n          if (i == 0 and self.keepBase) or \\\n             (i == t-1) or (i%2 == 1):\n              out_.append(layers_[i])\n        out = torch.cat(out_, 1)\n        return out\n\n\n\nclass TransitionUp(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        #print(\"upsample\",in_channels, out_channels)\n\n    def forward(self, x, skip, concat=True):\n        out = torch.nn.functional.interpolate(\n                x,\n                size=(skip.size(2), skip.size(3)),\n                mode=\"bilinear\",\n                align_corners=True,\n                            )\n        if concat:                            \n          out = torch.cat([out, skip], 1)\n          \n        return out\n\nclass hardnet(nn.Module):\n    def __init__(self, n_classes=19):\n        super(hardnet, self).__init__()\n\n        first_ch  = [16,24,32,48]\n        ch_list = [  64, 96, 160, 224, 320]\n        grmul = 1.7\n        gr       = [  10,16,18,24,32]\n        n_layers = [   4, 4, 8, 8, 8]\n\n        blks = len(n_layers) \n        self.shortcut_layers = []\n\n        self.base = nn.ModuleList([])\n        self.base.append (\n             ConvLayer(in_channels=3, out_channels=first_ch[0], kernel=3,\n                       stride=2) )\n        self.base.append ( ConvLayer(first_ch[0], first_ch[1],  kernel=3) )\n        self.base.append ( ConvLayer(first_ch[1], first_ch[2],  kernel=3, stride=2) )\n        self.base.append ( ConvLayer(first_ch[2], first_ch[3],  kernel=3) )\n\n        skip_connection_channel_counts = []\n        ch = first_ch[3]\n        for i in range(blks):\n            blk = HarDBlock(ch, gr[i], grmul, n_layers[i])\n            ch = blk.get_out_ch()\n            skip_connection_channel_counts.append(ch)\n            self.base.append ( blk )\n            if i < blks-1:\n              self.shortcut_layers.append(len(self.base)-1)\n\n            self.base.append ( ConvLayer(ch, ch_list[i], kernel=1) )\n            ch = ch_list[i]\n            \n            if i < blks-1:            \n              self.base.append ( nn.AvgPool2d(kernel_size=2, stride=2) )\n\n\n        cur_channels_count = ch\n        prev_block_channels = ch\n        n_blocks = blks-1\n        self.n_blocks =  n_blocks\n\n        #######################\n        #   Upsampling path   #\n        #######################\n\n        self.transUpBlocks = nn.ModuleList([])\n        self.denseBlocksUp = nn.ModuleList([])\n        self.conv1x1_up    = nn.ModuleList([])\n        \n        for i in range(n_blocks-1,-1,-1):\n            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n            self.conv1x1_up.append(ConvLayer(cur_channels_count, cur_channels_count//2, kernel=1))\n            cur_channels_count = cur_channels_count//2\n\n            blk = HarDBlock(cur_channels_count, gr[i], grmul, n_layers[i])\n            \n            self.denseBlocksUp.append(blk)\n            prev_block_channels = blk.get_out_ch()\n            cur_channels_count = prev_block_channels\n\n\n        self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n               out_channels=n_classes, kernel_size=1, stride=1,\n               padding=0, bias=True)\n    \n    def v2_transform(self, trt=False):        \n        for i in range( len(self.base)):\n            if isinstance(self.base[i], HarDBlock):\n                blk = self.base[i]\n                self.base[i] = HarDBlock_v2(blk.in_channels, blk.growth_rate, blk.grmul, blk.n_layers)\n                self.base[i].transform(blk, trt)\n\n        for i in range(self.n_blocks):\n            blk = self.denseBlocksUp[i]\n            self.denseBlocksUp[i] = HarDBlock_v2(blk.in_channels, blk.growth_rate, blk.grmul, blk.n_layers)\n            self.denseBlocksUp[i].transform(blk, trt)\n\n    def forward(self, x):\n        \n        skip_connections = []\n        size_in = x.size()\n        \n        \n        for i in range(len(self.base)):\n            x = self.base[i](x)\n            if i in self.shortcut_layers:\n                skip_connections.append(x)\n        out = x\n        \n        for i in range(self.n_blocks):\n            skip = skip_connections.pop()\n            out = self.transUpBlocks[i](out, skip, True)\n            out = self.conv1x1_up[i](out)\n            out = self.denseBlocksUp[i](out)\n        \n        out = self.finalConv(out)\n        \n        out = torch.nn.functional.interpolate(\n                            out,\n                            size=(size_in[2], size_in[3]),\n                            mode=\"bilinear\",\n                            align_corners=True)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:07:58.867464Z","iopub.status.busy":"2020-10-20T15:07:58.866557Z","iopub.status.idle":"2020-10-20T15:07:59.794057Z","shell.execute_reply":"2020-10-20T15:07:59.79251Z"},"papermill":{"duration":1.00346,"end_time":"2020-10-20T15:07:59.794267","exception":false,"start_time":"2020-10-20T15:07:58.790807","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as Fun\nimport torch \n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=0, size_average=True, ignore_index=255):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.ignore_index = ignore_index\n        self.size_average = size_average\n\n    def forward(self, inputs, targets):\n        ce_loss = Fun.cross_entropy(\n            inputs, targets, reduction='none', ignore_index=self.ignore_index)\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n        if self.size_average:\n            return focal_loss.mean()\n        else:\n            return focal_loss.sum()\n\n\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\nclass _StreamMetrics(object):\n    def __init__(self):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()\n\n    def update(self, gt, pred):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()\n\n    def get_results(self):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()\n\n    def to_str(self, metrics):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()\n\n    def reset(self):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()      \n\nclass StreamSegMetrics(_StreamMetrics):\n    \"\"\"\n    Stream Metrics for Semantic Segmentation Task\n    \"\"\"\n    def __init__(self, n_classes):\n        self.n_classes = n_classes\n        self.confusion_matrix = np.zeros((n_classes, n_classes))\n\n    def update(self, label_trues, label_preds):\n        #boolarr=label_trues==255\n        #label_preds[boolarr]=255\n        for lt, lp in zip(label_trues, label_preds):\n            self.confusion_matrix += self._fast_hist( lt.flatten(), lp.flatten() )\n    \n    @staticmethod\n    def to_str(results):\n        string = \"\\n\"\n        for k, v in results.items():\n            if k!=\"Class IoU\":\n                string += \"%s: %f\\n\"%(k, v)\n        \n        #string+='Class IoU:\\n'\n        #for k, v in results['Class IoU'].items():\n        #    string += \"\\tclass %d: %f\\n\"%(k, v)\n        return string\n\n    def _fast_hist(self, label_true, label_pred):\n        mask = (label_true >= 0) & (label_true < self.n_classes)\n        hist = np.bincount(\n            self.n_classes * label_true[mask].astype(int) + label_pred[mask],\n            minlength=self.n_classes ** 2,\n        ).reshape(self.n_classes, self.n_classes)\n        return hist\n\n    def get_results(self):\n        \"\"\"Returns accuracy score evaluation result.\n            - overall accuracy\n            - mean accuracy\n            - mean IU\n            - fwavacc\n        \"\"\"\n        hist = self.confusion_matrix\n        acc = np.diag(hist).sum() / hist.sum()\n        acc_cls = np.diag(hist) / hist.sum(axis=1)\n        acc_cls = np.nanmean(acc_cls)\n        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n        mean_iu = np.nanmean(iu)\n        freq = hist.sum(axis=1) / hist.sum()\n        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n        cls_iu = dict(zip(range(self.n_classes), iu))\n#         cls_ac = dict(zip(range(self.n_classes),np.diag(hist)/(hist.sum(axis=1))))\n\n        return {\n                \"Overall Acc\": acc,\n                \"Mean Acc\": acc_cls,\n                \"FreqW Acc\": fwavacc,\n                \"Mean IoU\": mean_iu,\n                #\"Class Acc\": cls_ac,\n                \"Class IoU\": cls_iu,\n            }\n        \n    def reset(self):\n        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n\nclass AverageMeter(object):\n    \"\"\"Computes average values\"\"\"\n    def __init__(self):\n        self.book = dict()\n\n    def reset_all(self):\n        self.book.clear()\n    \n    def reset(self, id):\n        item = self.book.get(id, None)\n        if item is not None:\n            item[0] = 0\n            item[1] = 0\n\n    def update(self, id, val):\n        record = self.book.get(id, None)\n        if record is None:\n            self.book[id] = [val, 1]\n        else:\n            record[0]+=val\n            record[1]+=1\n\n    def get_results(self, id):\n        record = self.book.get(id, None)\n        assert record is not None\n        return record[0] / record[1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import numpy as np\n# a=np.array([1,3,4,14,1,2,42,2])\n# boolarr=a==1\n# b=np.array([5,3,4,14,10,2,42,100])\n# b[boolarr]=255\n# print(b)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:07:59.972651Z","iopub.status.busy":"2020-10-20T15:07:59.971471Z","iopub.status.idle":"2020-10-20T15:07:59.976777Z","shell.execute_reply":"2020-10-20T15:07:59.978122Z"},"papermill":{"duration":0.103894,"end_time":"2020-10-20T15:07:59.978376","exception":false,"start_time":"2020-10-20T15:07:59.874482","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"_pil_interpolation_to_str = {\n    Image.NEAREST: 'PIL.Image.NEAREST',\n    Image.BILINEAR: 'PIL.Image.BILINEAR',\n    Image.BICUBIC: 'PIL.Image.BICUBIC',\n    Image.LANCZOS: 'PIL.Image.LANCZOS',\n    Image.HAMMING: 'PIL.Image.HAMMING',\n    Image.BOX: 'PIL.Image.BOX',\n}\nclass ExtResize(object):\n    \"\"\"Resize the input PIL Image to the given size.\n    Args:\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), output size will be matched to this. If size is an int,\n            smaller edge of the image will be matched to this number.\n            i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    \"\"\"\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        #assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img, lbl):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be scaled.\n        Returns:\n            PIL Image: Rescaled image.\n        \"\"\"\n        return F.resize(img, self.size, self.interpolation), F.resize(lbl, self.size, Image.NEAREST)\n\n    def __repr__(self):\n        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str) \n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:00.128352Z","iopub.status.busy":"2020-10-20T15:08:00.127132Z","iopub.status.idle":"2020-10-20T15:08:00.130339Z","shell.execute_reply":"2020-10-20T15:08:00.129357Z"},"papermill":{"duration":0.082684,"end_time":"2020-10-20T15:08:00.130524","exception":false,"start_time":"2020-10-20T15:08:00.04784","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/input/\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:00.278904Z","iopub.status.busy":"2020-10-20T15:08:00.277127Z","iopub.status.idle":"2020-10-20T15:08:00.995808Z","shell.execute_reply":"2020-10-20T15:08:00.995097Z"},"papermill":{"duration":0.794221,"end_time":"2020-10-20T15:08:00.995948","exception":false,"start_time":"2020-10-20T15:08:00.201727","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:01.100547Z","iopub.status.busy":"2020-10-20T15:08:01.099855Z","iopub.status.idle":"2020-10-20T15:08:01.105Z","shell.execute_reply":"2020-10-20T15:08:01.104409Z"},"papermill":{"duration":0.058574,"end_time":"2020-10-20T15:08:01.10512","exception":false,"start_time":"2020-10-20T15:08:01.046546","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"os.chdir(\"./script/\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:01.209746Z","iopub.status.busy":"2020-10-20T15:08:01.208849Z","iopub.status.idle":"2020-10-20T15:08:06.662626Z","shell.execute_reply":"2020-10-20T15:08:06.661988Z"},"papermill":{"duration":5.508463,"end_time":"2020-10-20T15:08:06.662755","exception":false,"start_time":"2020-10-20T15:08:01.154292","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"!python utils.py","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-10-20T15:08:06.759356Z","iopub.status.busy":"2020-10-20T15:08:06.758613Z","iopub.status.idle":"2020-10-20T15:08:06.896907Z","shell.execute_reply":"2020-10-20T15:08:06.895925Z"},"papermill":{"duration":0.18832,"end_time":"2020-10-20T15:08:06.897028","exception":false,"start_time":"2020-10-20T15:08:06.708708","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from utils import *","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:06.995438Z","iopub.status.busy":"2020-10-20T15:08:06.994535Z","iopub.status.idle":"2020-10-20T15:08:07.675186Z","shell.execute_reply":"2020-10-20T15:08:07.675729Z"},"papermill":{"duration":0.731583,"end_time":"2020-10-20T15:08:07.67588","exception":false,"start_time":"2020-10-20T15:08:06.944297","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"!ls\nos.chdir(\"..\")\nos.chdir('../working')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:08.000725Z","iopub.status.busy":"2020-10-20T15:08:07.984794Z","iopub.status.idle":"2020-10-20T15:08:08.025236Z","shell.execute_reply":"2020-10-20T15:08:08.024654Z"},"papermill":{"duration":0.100245,"end_time":"2020-10-20T15:08:08.025342","exception":false,"start_time":"2020-10-20T15:08:07.925097","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import json\nimport os\nfrom collections import namedtuple\n\nimport torch\nimport torch.utils.data as data\nfrom PIL import Image\nimport numpy as np\n\n\nclass CustomData(data.Dataset):\n\n    # Based on https://github.com/mcordts/cityscapesScripts\n    colorMap={\n        \"backgroud\": (225,229 , 204),\n        'wall':(152, 152, 79),\n        'building':(70, 70, 70),\n        'sky':(70, 130, 180),\n        'sidewalk':(244, 35, 232),\n        'field/grass':(152, 251, 152),\n        'vegitation':(107, 142, 35),\n        'person': (220, 20, 60),\n        'mountain':(139, 218, 51),\n        'stairs':(202, 251, 254),\n        'bench':(108, 246, 107),\n        'pole':(153, 153, 153),\n        'car':(41, 34, 177),\n        'bike':(111, 34, 177),\n        'animal':(211, 205, 33),\n        'ground':(147, 147, 136),\n        'fence':(241, 170, 17),\n        'water':(29, 231, 229),\n        'road':(35, 18, 16),\n        'sign_board':(113, 97, 41),\n        'floor':(73, 23, 77),\n        'traffic_light':(225, 175, 57),\n        'ceeling':(51, 0, 0),\n        'unlabelled':(0,0,0),\n        \n    }\n     \n    # Based on https://github.com/mcordts/cityscapesScripts\n    CustomDataSet = namedtuple('CustomDataSet', ['name', 'id', 'train_id', 'category', 'category_id',\n                                                     'has_instances', 'ignore', 'color'])\n    classes = [\n            CustomDataSet('backgroud',       0, 0, 'obstacle', 0, False, True, colorMap['backgroud']),\n            CustomDataSet('wall',            1, 1, 'solid', 0, False, True, colorMap['wall']),\n            CustomDataSet('building',        2, 2, 'solid', 0, False, True, colorMap['building']),\n            CustomDataSet('sky',             3, 3, 'backgroud', 0, False, True, colorMap['sky']),\n            CustomDataSet('sidewalk',        4, 4, 'nature', 0, False, True, colorMap['sidewalk']),\n            CustomDataSet('field/grass',     5, 5, 'nature', 0, False, True, colorMap['field/grass']),\n            CustomDataSet('vegitation',      6, 6, 'nature', 0, False, True, colorMap['vegitation']),\n            CustomDataSet('person',          7, 7, 'human', 0, False, True, colorMap['person']),\n            CustomDataSet('mountain',        8, 8, 'nature', 0, False, True, colorMap['mountain']),\n            CustomDataSet('stairs',          9, 255, 'solid', 0, False, False, colorMap['stairs']),\n            CustomDataSet('bench',           10, 0, 'obstacle', 0, False, False, colorMap['bench']),\n            CustomDataSet('pole',            11, 0, 'obstacle', 0, False, False, colorMap['pole']),\n            CustomDataSet('car',             12, 9, 'vahicle', 0, False, True, colorMap['car']),\n            CustomDataSet('bike',            13, 10, 'vahicle', 0, False, True, colorMap['bike']),\n            CustomDataSet('animal',          14, 11, 'animal', 0, False, True, colorMap['animal']),\n            CustomDataSet('ground',          15, 12, 'land', 0, False, True, colorMap['ground']),\n            CustomDataSet('fence',           16, 13, 'solid', 0, False, True, colorMap['fence']),\n            CustomDataSet('water',           17, 14, 'land', 0, False, True, colorMap['water']),\n            CustomDataSet('road',            18, 15, 'land', 0, False, True, colorMap['road']),\n            CustomDataSet('sign_board',      19, 0, 'obstacle', 0, False, False, colorMap['sign_board']),\n            CustomDataSet('floor',           20, 4, 'land', 0, False, False, colorMap['floor']),\n            CustomDataSet('traffic_light',   21, 0,'obstacle', 0, False, False, colorMap['traffic_light']),\n            CustomDataSet('ceeling',         22, 16, 'ceeling', 0, False, True, colorMap['ceeling']),\n            CustomDataSet('unlabelled',      23, 255, 'void', 0, False, False, colorMap['unlabelled']),\n            \n    ]\n    \n#     train_id_to_color = [c.color for c in classes if (c.train_id != -1 and c.train_id != 255)]\n#     train_id_to_color.append([0, 0, 0])\n#     train_id_to_color = np.array(train_id_to_color)\n    \n#     train_id_to_label= [c.name for c in classes if (c.train_id != -1 and c.train_id != 255)]\n#     train_id_to_label.append(\"unlabeled\")\n#     train_id_to_label = np.array(train_id_to_label)\n\n    train_id_to_color = [c.color for c in classes if (c.ignore)]\n    train_id_to_color.append([0, 0, 0])\n    train_id_to_color = np.array(train_id_to_color)\n\n    train_id_to_label= [c.name for c in classes if (c.ignore)]\n    train_id_to_label.append(\"unlabeled\")\n    train_id_to_label = np.array(train_id_to_label)\n    \n    \n    id_to_train_id = np.array([c.train_id for c in classes])\n    \n    def __init__(self, root_image, root_target, split='train', mode='fine', target_type='semantic', transform=None):        \n        \n        self.root_image = [os.path.expanduser(i) for i in root_image ]\n        self.root_target = [os.path.expanduser(i) for i in root_target]\n        \n        self.images = []\n        self.targets = []\n        self.transform = transform\n        self.split = split\n        \n        for i in range(len(self.root_image)):\n            self.root_image[i]=os.path.join(self.root_image[i],split)\n            self.root_target[i]=os.path.join(self.root_target[i],split)\n            print(self.root_image[i])\n            print(self.root_target[i])\n\n            lst=os.listdir(self.root_target[i])\n            \n            for img in lst:\n                if img[0]==\"C\":\n                    self.images.append(os.path.join(root_image[i],img[:-4]+'.jpg'))\n                else:\n                    self.images.append(os.path.join(self.root_image[i],img[:-4]+'.jpg'))\n                self.targets.append(os.path.join(self.root_target[i],img))\n        print(len(self.images))\n        print(len(self.targets))\n\n\n    @classmethod\n    def encode_target(cls, target):\n        return cls.id_to_train_id[np.array(target)]\n\n    @classmethod\n    def decode_target(cls, target):\n        target[target == 255] = 0\n        #target = target.astype('uint8') + 1\n        return cls.train_id_to_color[target]\n\n    def __getitem__(self, index):\n        image = Image.open(self.images[index]).convert('RGB')\n        target = Image.open(self.targets[index])\n        \n       \n        if self.transform:\n            image, target = self.transform(image, target)\n        target = self.encode_target(target)\n        #print(image.shape)\n        #print(target.shape)\n        \n        return image, target\n\n    def __len__(self):\n        return len(self.images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(CustomData.train_id_to_color)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CustomData.train_id_to_label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CustomData.id_to_train_id","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:08.131474Z","iopub.status.busy":"2020-10-20T15:08:08.130599Z","iopub.status.idle":"2020-10-20T15:08:08.13323Z","shell.execute_reply":"2020-10-20T15:08:08.133781Z"},"papermill":{"duration":0.062481,"end_time":"2020-10-20T15:08:08.133904","exception":false,"start_time":"2020-10-20T15:08:08.071423","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from torchvision.transforms.functional import normalize\nimport torch.nn as nn\nimport numpy as np\nimport os \n\ndef denormalize(tensor, mean, std):\n    mean = np.array(mean)\n    std = np.array(std)\n\n    _mean = -mean/std\n    _std = 1/std\n    return normalize(tensor, _mean, _std)\n\nclass Denormalize(object):\n    def __init__(self, mean, std):\n        mean = np.array(mean)\n        std = np.array(std)\n        self._mean = -mean/std\n        self._std = 1/std\n\n    def __call__(self, tensor):\n        if isinstance(tensor, np.ndarray):\n            return (tensor - self._mean.reshape(-1,1,1)) / self._std.reshape(-1,1,1)\n        return normalize(tensor, self._mean, self._std)\n\ndef set_bn_momentum(model, momentum=0.1):\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.momentum = momentum\n\ndef fix_bn(model):\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.eval()\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.mkdir(path)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:08.245967Z","iopub.status.busy":"2020-10-20T15:08:08.245106Z","iopub.status.idle":"2020-10-20T15:08:08.248272Z","shell.execute_reply":"2020-10-20T15:08:08.247689Z"},"papermill":{"duration":0.067845,"end_time":"2020-10-20T15:08:08.24839","exception":false,"start_time":"2020-10-20T15:08:08.180545","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from matplotlib import gridspec\n\ndef create_label_colormap():\n    colormap = CustomData.train_id_to_color\n    return colormap\n\n\ndef label_to_color_image(label):\n    if label.ndim != 2:\n        raise ValueError('Expect 2-D input label')\n\n    colormap = create_label_colormap()\n\n    if np.max(label) >= len(colormap):\n        raise ValueError('label value too large.')\n\n    return colormap[label]\n\n\ndef vis_segmentation(image, seg_map):\n    \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n    plt.figure(figsize=(20, 4))\n    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n\n    plt.subplot(grid_spec[0])\n    plt.imshow(image)\n    plt.axis('off')\n    plt.title('input image')\n\n    plt.subplot(grid_spec[1])\n    seg_image =  CustomData.decode_target(output_predictions.cpu()).astype(np.uint8)\n    plt.imshow(seg_image)\n    plt.axis('off')\n    plt.title('segmentation map')\n\n    plt.subplot(grid_spec[2])\n    plt.imshow(image)\n    plt.imshow(seg_image, alpha=0.7)\n    \n    plt.axis('off')\n    plt.title('segmentation overlay')\n\n    unique_labels = np.unique(seg_map)\n    print(\"Uniques Labels Found\",unique_labels)\n    ax = plt.subplot(grid_spec[3])\n    \n    \n    plt.imshow(FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n    ax.yaxis.tick_right()\n    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n    plt.xticks([], [])\n    ax.tick_params(width=0.0)\n    plt.grid('off')\n    plt.show()\n\n\nLABEL_NAMES = CustomData.train_id_to_label\n\nFULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\nFULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:08.348941Z","iopub.status.busy":"2020-10-20T15:08:08.347981Z","iopub.status.idle":"2020-10-20T15:08:08.351028Z","shell.execute_reply":"2020-10-20T15:08:08.350547Z"},"papermill":{"duration":0.056402,"end_time":"2020-10-20T15:08:08.351129","exception":false,"start_time":"2020-10-20T15:08:08.294727","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class opts:\n    dataset=\"cityscapes\"\n\n    data_root=\"../input/coco-dataset/Coco Stuff Dataset/imageLists\"\n  \n    imageFolder= [\n                  '../input/adek20-screen-parsing/ADEChallengeData2016/images',\n                  '../input/adek20-screen-parsing/ADEChallengeData2016/images',\n                  '../input/coco-dataset/Coco Stuff Dataset/images',\n                  '../input/custom-sidewalk/custom_sidewalk_updated/customdataset',\n                 ]\n    \n    targetFolder=[\n        '../input/adk-coco-filter/ADK_COCO_Filter_Anotation_Updated/adk',\n                  '../input/adk-coco-filter/ADK_COCO_Filter_Anotation_Updated/adk_floor_filter',\n                  '../input/adk-coco-filter/ADK_COCO_Filter_Anotation_Updated/coco',\n                  '../input/custom-sidewalk/custom_sidewalk_updated/customannotations',\n                 ]\n    lr=0.00001\n    weight_decay=0.0001\n    lr_policy= 'poly'  #learning rate scheduler policy\n    step_size=10000\n    saved_path=\"../input/fdnet-results-16-classes/Files/\"\n    pretrained=True\n    continue_training=True   #train previous model\n    batch_size=16\n    val_batch_size=16\n    num_classes=17\n    loss_function=\"cross_entropy\" #\"cross_entropy","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.045934,"end_time":"2020-10-20T15:08:08.442271","exception":false,"start_time":"2020-10-20T15:08:08.396337","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<!-- ### import numpy as np\n# from sklearn.metrics import confusion_matrix\n\n# class _StreamMetrics(object):\n#     def __init__(self):\n#         \"\"\" Overridden by subclasses \"\"\"\n#         raise NotImplementedError()\n\n#     def update(self, gt, pred):\n#         \"\"\" Overridden by subclasses \"\"\"\n#         raise NotImplementedError()\n\n#     def get_results(self):\n#         \"\"\" Overridden by subclasses \"\"\"\n#         raise NotImplementedError()\n\n#     def to_str(self, metrics):\n#         \"\"\" Overridden by subclasses \"\"\"\n#         raise NotImplementedError()\n\n#     def reset(self):\n#         \"\"\" Overridden by subclasses \"\"\"\n#         raise NotImplementedError()      \n\n# class StreamSegMetrics(_StreamMetrics):\n#     \"\"\"\n#     Stream Metrics for Semantic Segmentation Task\n#     \"\"\"\n#     def __init__(self, n_classes):\n#         self.n_classes = n_classes\n#         self.confusion_matrix = np.zeros((n_classes, n_classes))\n\n#     def update(self, label_trues, label_preds):\n#         for lt, lp in zip(label_trues, label_preds):\n#             self.confusion_matrix += self._fast_hist( lt.flatten(), lp.flatten() )\n    \n#     @staticmethod\n#     def to_str(results):\n#         string = \"\\n\"\n#         for k, v in results.items():\n#             if k!=\"Class IoU\":\n#                 string += \"%s: %f\\n\"%(k, v)\n        \n#         #string+='Class IoU:\\n'\n#         #for k, v in results['Class IoU'].items():\n#         #    string += \"\\tclass %d: %f\\n\"%(k, v)\n#         return string\n\n#     def _fast_hist(self, label_true, label_pred):\n#         mask = (label_true >= 0) & (label_true < self.n_classes)\n#         hist = np.bincount(\n#             self.n_classes * label_true[mask].astype(int) + label_pred[mask],\n#             minlength=self.n_classes ** 2,\n#         ).reshape(self.n_classes, self.n_classes)\n#         return hist\n\n#     def get_results(self):\n#         \"\"\"Returns accuracy score evaluation result.\n#             - overall accuracy\n#             - mean accuracy\n#             - mean IU\n#             - fwavacc\n#         \"\"\"\n#         hist = self.confusion_matrix\n#         acc = np.diag(hist).sum() / hist.sum()\n#         acc_cls = np.diag(hist) / hist.sum(axis=1)\n#         acc_cls = np.nanmean(acc_cls)\n#         iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n#         mean_iu = np.nanmean(iu)\n#         freq = hist.sum(axis=1) / hist.sum()\n#         fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n#         cls_iu = dict(zip(range(self.n_classes), iu))\n\n#         return {\n#                 \"Overall Acc\": acc,\n#                 \"Mean Acc\": acc_cls,\n#                 \"FreqW Acc\": fwavacc,\n#                 \"Mean IoU\": mean_iu,\n#                 \"Class IoU\": cls_iu,\n#             }\n        \n#     def reset(self):\n#         self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n\n# class AverageMeter(object):\n#     \"\"\"Computes average values\"\"\"\n#     def __init__(self):\n#         self.book = dict()\n\n#     def reset_all(self):\n#         self.book.clear()\n    \n#     def reset(self, id):\n#         item = self.book.get(id, None)\n#         if item is not None:\n#             item[0] = 0\n#             item[1] = 0\n\n#     def update(self, id, val):\n#         record = self.book.get(id, None)\n#         if record is None:\n#             self.book[id] = [val, 1]\n#         else:\n#             record[0]+=val\n#             record[1]+=1\n\n#     def get_results(self, id):\n#         record = self.book.get(id, None)\n#         assert record is not None\n#         return record[0] / record[1] -->"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:08.544189Z","iopub.status.busy":"2020-10-20T15:08:08.543364Z","iopub.status.idle":"2020-10-20T15:08:08.546441Z","shell.execute_reply":"2020-10-20T15:08:08.546928Z"},"papermill":{"duration":0.058479,"end_time":"2020-10-20T15:08:08.547053","exception":false,"start_time":"2020-10-20T15:08:08.488574","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\ndef get_dataset(opts):\n    \"\"\" Dataset And Augmentation\n    \"\"\"\n    if opts.dataset == 'cityscapes':\n        train_transform = ExtCompose([\n            #et.ExtResize( 512 ),\n            ExtResize((640,480)),\n            #ExtRandomCrop(size=(opts.crop_size, opts.crop_size),pad_if_needed=True),\n            ExtColorJitter( brightness=0.5, contrast=0.5, saturation=0.5 ),\n            ExtRandomHorizontalFlip(),\n            ExtToTensor(),\n            ExtNormalize(mean=[0.485, 0.456, 0.406],\n                            std=[0.229, 0.224, 0.225]),\n        ])\n        val_transform = ExtCompose([\n            #et.ExtResize( 512 ),\n            ExtResize((640,480)),\n            ExtColorJitter( brightness=0.5, contrast=0.5, saturation=0.5 ),\n            #ExtRandomCrop(size=(opts.crop_size, opts.crop_size)),\n            ExtToTensor(),\n            ExtNormalize(mean=[0.485, 0.456, 0.406],\n                            std=[0.229, 0.224, 0.225]),\n        ])\n\n        train_dst = CustomData(root_image=opts.imageFolder,root_target=opts.targetFolder,\n                               split='training', transform=train_transform)\n        val_dst = CustomData(root_image=opts.imageFolder,root_target=opts.targetFolder,\n                             split='validation', transform=val_transform)\n    return train_dst, val_dst","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:08.644548Z","iopub.status.busy":"2020-10-20T15:08:08.643921Z","iopub.status.idle":"2020-10-20T15:08:09.629155Z","shell.execute_reply":"2020-10-20T15:08:09.628353Z"},"papermill":{"duration":1.036162,"end_time":"2020-10-20T15:08:09.629305","exception":false,"start_time":"2020-10-20T15:08:08.593143","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_dst, val_dst = get_dataset(opts)\ntrain_loader = data.DataLoader(train_dst, batch_size=opts.batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = data.DataLoader(val_dst, batch_size=opts.val_batch_size, shuffle=True, num_workers=4)\nprint(\"Dataset: %s, Train set: %d, Val set: %d\" % (opts.dataset, len(train_dst), len(val_dst)))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:10.091882Z","iopub.status.busy":"2020-10-20T15:08:10.091105Z","iopub.status.idle":"2020-10-20T15:08:16.709957Z","shell.execute_reply":"2020-10-20T15:08:16.711375Z"},"papermill":{"duration":7.035115,"end_time":"2020-10-20T15:08:16.711573","exception":false,"start_time":"2020-10-20T15:08:09.676458","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"dataset_iter = iter(train_loader)\nimg,lal=dataset_iter.next()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:17.297631Z","iopub.status.busy":"2020-10-20T15:08:17.296715Z","iopub.status.idle":"2020-10-20T15:08:17.302907Z","shell.execute_reply":"2020-10-20T15:08:17.303888Z"},"papermill":{"duration":0.077534,"end_time":"2020-10-20T15:08:17.304126","exception":false,"start_time":"2020-10-20T15:08:17.226592","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Device: %s\" % device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=hardnet(19)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dfs_freeze(model):\n    for name, child in model.named_children():\n        for param in child.parameters():\n            param.requires_grad = False\n        dfs_freeze(child)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:18.856041Z","iopub.status.busy":"2020-10-20T15:08:18.855046Z","iopub.status.idle":"2020-10-20T15:08:18.858063Z","shell.execute_reply":"2020-10-20T15:08:18.857497Z"},"papermill":{"duration":0.059116,"end_time":"2020-10-20T15:08:18.858162","exception":false,"start_time":"2020-10-20T15:08:18.799046","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def save_ckpt(path):\n    \"\"\" save current model\n    \"\"\"\n    torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n            \"scheduler_state\": scheduler.state_dict(),\n            \"best_score\": best_score,\n    }, path)\n    print(\"Model saved as %s\" % path)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.051546,"end_time":"2020-10-20T15:08:19.066917","exception":false,"start_time":"2020-10-20T15:08:19.015371","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.SGD(model.parameters(), lr=opts.lr, momentum=0.9, weight_decay=opts.weight_decay)\nmetrics = StreamSegMetrics(opts.num_classes)\ncreterion=\"\"\nif opts.loss_function==\"focal\":\n    criterion = FocalLoss(ignore_index=255, size_average=True)\nelse:\n    criterion = nn.CrossEntropyLoss(ignore_index=255, reduction='mean')\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=opts.step_size, gamma=0.1)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T15:08:19.184107Z","iopub.status.busy":"2020-10-20T15:08:19.183394Z","iopub.status.idle":"2020-10-20T15:08:19.816248Z","shell.execute_reply":"2020-10-20T15:08:19.81521Z"},"papermill":{"duration":0.699013,"end_time":"2020-10-20T15:08:19.816385","exception":false,"start_time":"2020-10-20T15:08:19.117372","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\n#criterion.to(device)\n\ncur_epoch=0\nbest_score=0\nLoss=[]\nScore=[]\n\nif opts.pretrained:\n    model.finalConv= nn.Conv2d(48, opts.num_classes, 1, 1, bias=True) \n    ckpt=opts.saved_path+\"last_model.pth\"\n    checkpoint = torch.load(ckpt, map_location=torch.device('cpu'))\n    model.load_state_dict(checkpoint[\"model_state\"])\n    #model = nn.DataParallel(model)\n    model.to(device)\n    with open(opts.saved_path+\"Loss.txt\",\"rb\") as File:\n        Loss = pickle.load(File)\n    with open(opts.saved_path+\"Score.txt\",\"rb\") as File:\n        Score = pickle.load(File)\n        \n    if opts.continue_training:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n        scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n        cur_epoch = checkpoint[\"epoch\"]\n        best_score = checkpoint['best_score']\n        print(\"Training state restored from %s\" % opts.saved_path)\n    print(\"Model restored from %s\" % opts.saved_path)\n    del checkpoint  # free memory\nelse:\n    model = torch.nn.DataParallel(model)\n    checkpoint=torch.load('../input/cityscape-mobilenet/hardnet70_cityscapes_model_2.pkl',map_location=torch.device('cpu'))\n    model.load_state_dict(checkpoint[\"model_state\"])\n    model=model.module\n    model.finalConv= nn.Conv2d(48, opts.num_classes, 1, 1, bias=True) \n    #model = nn.DataParallel(model)\n    model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(Loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cur_epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef validate1(opts, model,loader,metrics,device):\n    \n    metrics.reset()\n    #validation_loss = []\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(loader):\n            images = images.to(device, dtype=torch.float32)\n            labels = labels.to(device, dtype=torch.long)\n\n            outputs = model(images)\n            preds = outputs.detach().max(dim=1)[1].cpu().numpy()\n            targets=labels.cpu().numpy()\n            #loss = criterion(outputs, labels)\n            \n            \n            metrics.update(targets, preds)\n            #np_loss=loss.detach().cpu().numpy() \n            #validation_loss.append(np_loss)\n    \n    #loss=np.mean(validation_loss)\n    #print(validation_loss)\n    return metrics.get_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mkdir(\"./Files\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interval_loss = 0\nepochs=cur_epoch+58\nmax_score=10000000000\nmodel_curve=[]\n\n\n\nbest_score = 0.0\ncur_itrs=0\nstart = perf_counter()\nfor epoch in tqdm(range(cur_epoch+1,epochs), desc=\"Epochs\"):\n    model.train()\n    running_loss = []\n    for step, (images, labels) in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n        cur_itrs += 1\n        #print(images.shape)\n        #print(labels.shape)\n        \n        images = images.to(device, dtype=torch.float32)\n        labels = labels.to(device, dtype=torch.long)\n        optimizer.zero_grad()\n\n        outputs = model(images)\n        #print(outputs.shape)\n        \n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        #end = perf_counter()\n        \n        running_loss.append(loss.item())\n        interval_loss += loss.item()\n\n    model.eval()\n    val_score = validate1(opts=opts, model=model, loader=val_loader, metrics=metrics,device=device)\n    print(\"Training Loss\",np.mean(running_loss),\"Validation Loss\")\n    print(metrics.to_str(val_score))\n\n    #if val_score['Mean IoU'] > best_score:  # save best model\n    best_score = val_score['Mean IoU']\n    save_ckpt('./Files/last_model.pth')\n#     else:\n#         print(\"Stopping Condition Occoured\")\n    Loss.append(np.mean(running_loss))\n    Score.append(val_score)\n    \n    with open(\"./Files/Loss.txt\",\"wb\") as File:\n        pickle.dump(Loss,File)\n    with open(\"./Files/Score.txt\",\"wb\") as File:\n        pickle.dump(Score,File)\n\n    scheduler.step()\n    print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch+1, epochs, np.mean(running_loss)))\nend = perf_counter()\nprint(\"Time\",end-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(Score[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def zipdir(path, ziph):\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            ziph.write(os.path.join(root, file))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"zipf = zipfile.ZipFile(\"./Files.zip\", 'w', zipfile.ZIP_DEFLATED)\nzipdir('./Files', zipf)\nzipf.close()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.067632,"end_time":"2020-10-20T21:20:29.61985","exception":false,"start_time":"2020-10-20T21:20:29.552218","status":"completed"},"tags":[]},"cell_type":"markdown","source":""},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T21:20:29.76308Z","iopub.status.busy":"2020-10-20T21:20:29.762286Z","iopub.status.idle":"2020-10-20T21:20:29.765718Z","shell.execute_reply":"2020-10-20T21:20:29.765129Z"},"papermill":{"duration":0.076081,"end_time":"2020-10-20T21:20:29.765814","exception":false,"start_time":"2020-10-20T21:20:29.689733","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom torchvision import transforms\n\nfrom PIL import Image\nimport requests","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T21:20:29.918921Z","iopub.status.busy":"2020-10-20T21:20:29.918011Z","iopub.status.idle":"2020-10-20T21:20:31.305125Z","shell.execute_reply":"2020-10-20T21:20:31.305714Z"},"papermill":{"duration":1.470414,"end_time":"2020-10-20T21:20:31.305872","exception":false,"start_time":"2020-10-20T21:20:29.835458","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#start1 = perf_counter()\ninput_image = Image.open(\"../input/my-dataset/images/6.jpg\")\n#url=\"https://images.squarespace-cdn.com/content/v1/5c9b3ac2e5f7d1493cede23f/1554710179935-T2AY655VMKGCZ6V5D0AE/ke17ZwdGBToddI8pDm48kBqZKGEd5IotpaYmRYDAsV97gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UYXHvO3AlG5qAFDx5BBWbEnTecsx1VxL9XkiyXErq2cftN_KfIuurh-w-x7bjNuMJA/london-citizen-m-stairs-15.jpg\"\n#input_image=Image.open(requests.get(url, stream=True).raw)\npreprocess_input= transforms.Compose([\n    transforms.Resize((480,640)),\n])\npreprocess = transforms.Compose([\n    transforms.Resize((480,640)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    #transforms.ColorJitter( brightness=0.5, contrast=0.5, saturation=0.5 ),\n])\ninput_image=preprocess_input(input_image)\ninput_tensor= preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n    input_batch = input_batch.to('cuda')\n    model.to('cuda')\n\nmodel.eval()\nwith torch.no_grad():\n    start = perf_counter()\n    output = model(input_batch)[0]\n    end = perf_counter()\n    print(end-start)\n\noutput_predictions = output.argmax(0)\npred = CustomData.decode_target(output_predictions.cpu()).astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-20T21:20:31.449068Z","iopub.status.busy":"2020-10-20T21:20:31.447836Z","iopub.status.idle":"2020-10-20T21:20:32.485846Z","shell.execute_reply":"2020-10-20T21:20:32.486358Z"},"papermill":{"duration":1.112124,"end_time":"2020-10-20T21:20:32.486496","exception":false,"start_time":"2020-10-20T21:20:31.374372","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"vis_segmentation(input_image, output_predictions.cpu())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess = transforms.Compose([\n    transforms.Resize((480,640)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ndef run_model(input_image):\n    input_tensor= preprocess(input_image)\n    input_batch = input_tensor.unsqueeze(0) \n    if torch.cuda.is_available():\n        input_batch = input_batch.to('cuda')\n        model.to('cuda')\n    model.eval()\n    with torch.no_grad():\n        output = model(input_batch)[0]\n    output_predictions = output.argmax(0)\n    return output_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport urllib\nimport IPython\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nimport io\nimport pickle\n\nnp_buff=[]\n\ndef vis_segmentation_stream(image, seg_map, index):\n    \n    \"\"\"Visualizes segmentation overlay view and stream it with IPython display.\"\"\"\n#     fig=plt.figure(figsize=(12, 7))\n    seg_image = CustomData.decode_target(seg_map.cpu()).astype(np.uint8)\n    seg_image=Image.fromarray(seg_image.astype('uint8'), 'RGB')\n\n    background = image.convert(\"RGBA\")\n    overlay = seg_image.convert(\"RGBA\")\n\n    new_img = Image.blend(background, overlay, 0.7)\n    #new_img.save(\"new.png\",\"PNG\")\n    img=np.array(new_img)\n    #print(img.shape)\n    np_buff.append(img)\n\n\ndef run_visualization_video(frame, index):\n    \"\"\"Inferences DeepLab model on a video file and stream the visualization.\"\"\"\n    #frame=preprocess_input(frame)\n    original_im = Image.fromarray(frame[..., ::-1])\n    original_im=preprocess_input(original_im)\n    seg_map = run_model(original_im)\n    vis_segmentation_stream(original_im, seg_map, index)\n\n\nSAMPLE_VIDEO = '../input/video-test/xyz.mp4'\nif not os.path.isfile(SAMPLE_VIDEO): \n    print('downloading the sample video...')\n    SAMPLE_VIDEO = urllib.request.urlretrieve('https://github.com/lexfridman/mit-deep-learning/raw/master/tutorial_driving_scene_segmentation/mit_driveseg_sample.mp4')[0]\nprint('running deeplab on the sample video...')\n\nvideo = cv2.VideoCapture(SAMPLE_VIDEO)\n#print(len(video))\n\n# # num_frames = 598  # uncomment to use the full sample video\nnum_frames = 20000\n\ntry:\n    start = perf_counter()\n    for i in range(num_frames):\n        _, frame = video.read()\n        if not _: break\n        if i%15==0:\n            run_visualization_video(frame, i)\n    end = perf_counter()\n    sec=end-start\n    print(sec/60)\nexcept KeyboardInterrupt:\n    plt.close()\n    print(\"Stream stopped.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"video=cv2.VideoWriter(\"result.mp4\",cv2.VideoWriter_fourcc(*'DIVX'), 15,(np_buff[0].shape[1],np_buff[0].shape[0]))\nfor i in np_buff:\n    i=i[:,:,:-1]\n    video.write(i[:,:,::-1])\nvideo.release()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# val_files= os.listdir('../input/floor-separated/Annotation_adk_coco_only/adk/validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}