{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Including Dependencies"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T07:12:37.078688Z","iopub.status.busy":"2020-12-02T07:12:37.07766Z","iopub.status.idle":"2020-12-02T07:12:38.856058Z","shell.execute_reply":"2020-12-02T07:12:38.854429Z"},"papermill":{"duration":1.844091,"end_time":"2020-12-02T07:12:38.856194","exception":false,"start_time":"2020-12-02T07:12:37.012103","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"!pip install efficientunet-pytorch\nfrom efficientunet import *\nfrom tqdm.notebook import tqdm\nimport os\nimport random\nimport argparse\nimport numpy as np\nfrom torch.utils import data\nfrom time import perf_counter\nimport time\nimport torch\nimport torch.nn as nn\nfrom threading import Thread\nimport IPython\n\nimport json\nimport os\nfrom collections import namedtuple\n\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport numpy as np\n\nfrom PIL import Image\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\nimport pickle\nimport zipfile\nimport torch.nn as nn\nimport torch.nn.functional as Fun\nimport torch\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom torchvision import transforms\nimport requests\n\nimport cv2\nimport urllib\nimport IPython\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nimport io\nimport pickle\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utility Functions"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T07:12:39.108825Z","iopub.status.busy":"2020-12-02T07:12:39.093181Z","iopub.status.idle":"2020-12-02T07:12:40.240467Z","shell.execute_reply":"2020-12-02T07:12:40.239566Z"},"papermill":{"duration":1.220612,"end_time":"2020-12-02T07:12:40.24066","exception":false,"start_time":"2020-12-02T07:12:39.020048","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#Focal Loss Code Used For Semantic Segmentation, Reference Mentioned in Bottom of the Notebook\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=0, size_average=True, ignore_index=255):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.ignore_index = ignore_index\n        self.size_average = size_average\n\n    def forward(self, inputs, targets):\n        ce_loss = Fun.cross_entropy(\n            inputs, targets, reduction='none', ignore_index=self.ignore_index)\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n        if self.size_average:\n            return focal_loss.mean()\n        else:\n            return focal_loss.sum()\n\n#Matrix For Evaluation, Reference Mentioned in Bottom of the Notebook\nclass _StreamMetrics(object):\n    def __init__(self):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()\n\n    def update(self, gt, pred):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()\n\n    def get_results(self):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()\n\n    def to_str(self, metrics):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()\n\n    def reset(self):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()      \n\nclass StreamSegMetrics(_StreamMetrics):\n    \"\"\"\n    Stream Metrics for Semantic Segmentation Task\n    \"\"\"\n    def __init__(self, n_classes):\n        self.n_classes = n_classes\n        self.confusion_matrix = np.zeros((n_classes, n_classes))\n\n    def update(self, label_trues, label_preds):\n        #boolarr=label_trues==255\n        #label_preds[boolarr]=255\n        for lt, lp in zip(label_trues, label_preds):\n            self.confusion_matrix += self._fast_hist( lt.flatten(), lp.flatten() )\n    \n    @staticmethod\n    def to_str(results):\n        string = \"\\n\"\n        for k, v in results.items():\n            if k!=\"Class IoU\":\n                string += \"%s: %f\\n\"%(k, v)\n        \n        #string+='Class IoU:\\n'\n        #for k, v in results['Class IoU'].items():\n        #    string += \"\\tclass %d: %f\\n\"%(k, v)\n        return string\n\n    def _fast_hist(self, label_true, label_pred):\n        mask = (label_true >= 0) & (label_true < self.n_classes)\n        hist = np.bincount(\n            self.n_classes * label_true[mask].astype(int) + label_pred[mask],\n            minlength=self.n_classes ** 2,\n        ).reshape(self.n_classes, self.n_classes)\n        return hist\n\n    def get_results(self):\n        \"\"\"Returns accuracy score evaluation result.\n            - overall accuracy\n            - mean accuracy\n            - mean IU\n            - fwavacc\n        \"\"\"\n        hist = self.confusion_matrix\n        acc = np.diag(hist).sum() / hist.sum()\n        acc_cls = np.diag(hist) / hist.sum(axis=1)\n        acc_cls = np.nanmean(acc_cls)\n        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n        mean_iu = np.nanmean(iu)\n        freq = hist.sum(axis=1) / hist.sum()\n        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n        cls_iu = dict(zip(range(self.n_classes), iu))\n#         cls_ac = dict(zip(range(self.n_classes),np.diag(hist)/(hist.sum(axis=1))))\n\n        return {\n                \"Overall Acc\": acc,\n                \"Mean Acc\": acc_cls,\n                \"FreqW Acc\": fwavacc,\n                \"Mean IoU\": mean_iu,\n                #\"Class Acc\": cls_ac,\n                \"Class IoU\": cls_iu,\n            }\n        \n    def reset(self):\n        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n\nclass AverageMeter(object):\n    \"\"\"Computes average values\"\"\"\n    def __init__(self):\n        self.book = dict()\n\n    def reset_all(self):\n        self.book.clear()\n    \n    def reset(self, id):\n        item = self.book.get(id, None)\n        if item is not None:\n            item[0] = 0\n            item[1] = 0\n\n    def update(self, id, val):\n        record = self.book.get(id, None)\n        if record is None:\n            self.book[id] = [val, 1]\n        else:\n            record[0]+=val\n            record[1]+=1\n\n    def get_results(self, id):\n        record = self.book.get(id, None)\n        assert record is not None\n        return record[0] / record[1]\n\n_pil_interpolation_to_str = {\n    Image.NEAREST: 'PIL.Image.NEAREST',\n    Image.BILINEAR: 'PIL.Image.BILINEAR',\n    Image.BICUBIC: 'PIL.Image.BICUBIC',\n    Image.LANCZOS: 'PIL.Image.LANCZOS',\n    Image.HAMMING: 'PIL.Image.HAMMING',\n    Image.BOX: 'PIL.Image.BOX',\n}\nclass ExtResize(object):\n    \"\"\"Resize the input PIL Image to the given size.\n    Args:\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), output size will be matched to this. If size is an int,\n            smaller edge of the image will be matched to this number.\n            i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    \"\"\"\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        #assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img, lbl):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be scaled.\n        Returns:\n            PIL Image: Rescaled image.\n        \"\"\"\n        return F.resize(img, self.size, self.interpolation), F.resize(lbl, self.size, Image.NEAREST)\n\n    def __repr__(self):\n        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Utils files Model"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T07:12:40.793925Z","iopub.status.busy":"2020-12-02T07:12:40.793119Z","iopub.status.idle":"2020-12-02T07:12:40.797412Z","shell.execute_reply":"2020-12-02T07:12:40.796685Z"},"papermill":{"duration":0.096536,"end_time":"2020-12-02T07:12:40.797526","exception":false,"start_time":"2020-12-02T07:12:40.70099","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#This cell is used for loading Model and Utils File from Script, Model is Directly Loaded From the Command\n# !pip install efficientunet-pytorch at top of this notbook\n\nos.chdir(\"/kaggle/input/script/\")\n!python utils.py\n\nfrom utils import *\n\n!ls\nos.chdir(\"..\")\nos.chdir('../working')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DataLoader"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T07:12:50.063844Z","iopub.status.busy":"2020-12-02T07:12:50.037805Z","iopub.status.idle":"2020-12-02T07:12:50.092082Z","shell.execute_reply":"2020-12-02T07:12:50.091439Z"},"papermill":{"duration":0.119755,"end_time":"2020-12-02T07:12:50.092219","exception":false,"start_time":"2020-12-02T07:12:49.972464","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import json\nimport os\nfrom collections import namedtuple\n\nimport torch\nimport torch.utils.data as data\nfrom PIL import Image\nimport numpy as np\n\n\nclass CustomData(data.Dataset):\n\n    colorMap={\n        \"backgroud\": (225,229 , 204),\n        'wall':(152, 152, 79),\n        'building':(70, 70, 70),\n        'sky':(70, 130, 180),\n        'sidewalk':(244, 35, 232),\n        'field/grass':(152, 251, 152),\n        'vegitation':(107, 142, 35),\n        'person': (220, 20, 60),\n        'mountain':(139, 218, 51),\n        'stairs':(202, 251, 254),\n        'bench':(108, 246, 107),\n        'pole':(153, 153, 153),\n        'car':(41, 34, 177),\n        'bike':(111, 34, 177),\n        'animal':(211, 205, 33),\n        'ground':(147, 147, 136),\n        'fence':(241, 170, 17),\n        'water':(29, 231, 229),\n        'road':(35, 18, 16),\n        'sign_board':(113, 97, 41),\n        'floor':(73, 23, 77),\n        'traffic_light':(225, 175, 57),\n        'ceeling':(51, 0, 0),\n        'unlabelled':(0,0,0),\n        \n    }\n     \n    CustomDataSet = namedtuple('CustomDataSet', ['name', 'id', 'train_id', 'category', 'category_id',\n                                                     'has_instances', 'ignore', 'color'])\n    classes = [\n            CustomDataSet('backgroud',       0, 0, 'obstacle', 0, False, True, colorMap['backgroud']),\n            CustomDataSet('wall',            1, 1, 'solid', 0, False, True, colorMap['wall']),\n            CustomDataSet('building',        2, 2, 'solid', 0, False, True, colorMap['building']),\n            CustomDataSet('sky',             3, 3, 'backgroud', 0, False, True, colorMap['sky']),\n            CustomDataSet('sidewalk',        4, 4, 'nature', 0, False, True, colorMap['sidewalk']),\n            CustomDataSet('field/grass',     5, 5, 'nature', 0, False, True, colorMap['field/grass']),\n            CustomDataSet('vegitation',      6, 6, 'nature', 0, False, True, colorMap['vegitation']),\n            CustomDataSet('person',          7, 7, 'human', 0, False, True, colorMap['person']),\n            CustomDataSet('mountain',        8, 8, 'nature', 0, False, True, colorMap['mountain']),\n            CustomDataSet('stairs',          9, 255, 'solid', 0, False, False, colorMap['stairs']),\n            CustomDataSet('bench',           10, 0, 'obstacle', 0, False, False, colorMap['bench']),\n            CustomDataSet('pole',            11, 0, 'obstacle', 0, False, False, colorMap['pole']),\n            CustomDataSet('car',             12, 9, 'vahicle', 0, False, True, colorMap['car']),\n            CustomDataSet('bike',            13, 10, 'vahicle', 0, False, True, colorMap['bike']),\n            CustomDataSet('animal',          14, 11, 'animal', 0, False, True, colorMap['animal']),\n            CustomDataSet('ground',          15, 12, 'land', 0, False, True, colorMap['ground']),\n            CustomDataSet('fence',           16, 13, 'solid', 0, False, True, colorMap['fence']),\n            CustomDataSet('water',           17, 14, 'land', 0, False, True, colorMap['water']),\n            CustomDataSet('road',            18, 15, 'land', 0, False, True, colorMap['road']),\n            CustomDataSet('sign_board',      19, 0, 'obstacle', 0, False, False, colorMap['sign_board']),\n            CustomDataSet('floor',           20, 4, 'land', 0, False, False, colorMap['floor']),\n            CustomDataSet('traffic_light',   21, 0,'obstacle', 0, False, False, colorMap['traffic_light']),\n            CustomDataSet('ceeling',         22, 16, 'ceeling', 0, False, True, colorMap['ceeling']),\n            CustomDataSet('unlabelled',      23, 255, 'void', 0, False, False, colorMap['unlabelled']),\n            \n    ]\n    \n    #Numpy Array used for changing output to color Images\n    train_id_to_color = [c.color for c in classes if (c.ignore)]\n    train_id_to_color.append([0, 0, 0])\n    train_id_to_color = np.array(train_id_to_color)\n\n    #Gives Labels of Output\n    train_id_to_label= [c.name for c in classes if (c.ignore)]\n    train_id_to_label.append(\"unlabeled\")\n    train_id_to_label = np.array(train_id_to_label)\n    id_to_train_id = np.array([c.train_id for c in classes])\n     \n    def __init__(self, root_image, root_target, split='train', mode='fine', target_type='semantic', transform=None):        \n        \n           \n        \"\"\"\n        Get All Images and Anotation paths from the List of \n        directories mentioned in root_images and root_target respestively and Store individual images and targets \n        paths to self.images and self.target List\n        \n        \"\"\" \n    \n        self.root_image = [os.path.expanduser(i) for i in root_image ]\n        self.root_target = [os.path.expanduser(i) for i in root_target]\n        \n        self.images = []\n        self.targets = []\n        self.transform = transform\n        self.split = split\n        \n        for i in range(len(self.root_image)):\n            self.root_image[i]=os.path.join(self.root_image[i],split)\n            self.root_target[i]=os.path.join(self.root_target[i],split)\n            print(self.root_image[i])\n            print(self.root_target[i])\n\n            lst=os.listdir(self.root_target[i])\n            \n            for img in lst:\n                if img[0]==\"C\":\n                    self.images.append(os.path.join(root_image[i],img[:-4]+'.jpg'))\n                else:\n                    self.images.append(os.path.join(self.root_image[i],img[:-4]+'.jpg'))\n                self.targets.append(os.path.join(self.root_target[i],img))\n        print(len(self.images))\n        print(len(self.targets))\n\n\n    @classmethod\n    def encode_target(cls, target):\n        \"\"\"\n            input: target image 2d matrix \n            output: 2d matrix with custom mapping\n        \"\"\"\n        return cls.id_to_train_id[np.array(target)]\n\n    @classmethod\n    def decode_target(cls, target):\n        \"\"\"\n            input: target 2d matrix\n            output: color map (RGB) image\n        \"\"\"\n        target[target == 255] = 0\n        #target = target.astype('uint8') + 1\n        return cls.train_id_to_color[target]\n\n    def __getitem__(self, index):\n        \"\"\"\n            input: index of an image and target file\n            Opens the Image from the paths\n            Apply Trasformation\n            Return images and target(Anotation)\n        \"\"\"\n        image = Image.open(self.images[index]).convert('RGB')\n        target = Image.open(self.targets[index])\n        \n       \n        if self.transform:\n            image, target = self.transform(image, target)\n        target = self.encode_target(target)\n        #print(image.shape)\n        #print(target.shape)\n        \n        return image, target\n\n    def __len__(self):\n        return len(self.images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Setup"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T07:12:50.350547Z","iopub.status.busy":"2020-12-02T07:12:50.349402Z","iopub.status.idle":"2020-12-02T07:12:50.366827Z","shell.execute_reply":"2020-12-02T07:12:50.366174Z"},"papermill":{"duration":0.085468,"end_time":"2020-12-02T07:12:50.366948","exception":false,"start_time":"2020-12-02T07:12:50.28148","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from matplotlib import gridspec\n\ndef create_label_colormap():\n    colormap = CustomData.train_id_to_color\n    return colormap\n\n\ndef label_to_color_image(label):\n    if label.ndim != 2:\n        raise ValueError('Expect 2-D input label')\n\n    colormap = create_label_colormap()\n\n    if np.max(label) >= len(colormap):\n        raise ValueError('label value too large.')\n\n    return colormap[label]\n\n\ndef vis_segmentation(image,seg_map):\n    \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n    plt.figure(figsize=(20, 4))\n    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n\n    plt.subplot(grid_spec[0])\n    plt.imshow(image)\n    plt.axis('off')\n    plt.title('input image')\n\n    plt.subplot(grid_spec[1])\n    seg_image =  CustomData.decode_target(seg_map.cpu()).astype(np.uint8)\n    plt.imshow(seg_image)\n    plt.axis('off')\n    plt.title('segmentation map')\n\n    plt.subplot(grid_spec[2])\n    plt.imshow(image)\n    plt.imshow(seg_image, alpha=0.7)\n    \n    plt.axis('off')\n    plt.title('segmentation overlay')\n\n    unique_labels = np.unique(seg_map)\n    print(\"Uniques Labels Found\",unique_labels)\n    ax = plt.subplot(grid_spec[3])\n    \n    \n    plt.imshow(FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n    ax.yaxis.tick_right()\n    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n    plt.xticks([], [])\n    ax.tick_params(width=0.0)\n    plt.grid('off')\n    plt.show()\n\n\nLABEL_NAMES = CustomData.train_id_to_label\n\nFULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\nFULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Get Datatset\ndef get_dataset(imageFolders,targetFolders,batchSize,valBatchSize):\n    \"\"\" Dataset And Augmentation\n    \"\"\"\n    train_transform = ExtCompose([\n        ExtResize((256,256)),\n        ExtColorJitter( brightness=0.5, contrast=0.5, saturation=0.5 ),\n        ExtRandomHorizontalFlip(),\n        ExtToTensor(),\n        ExtNormalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225]),\n    ])\n    val_transform = ExtCompose([\n        ExtResize((256,256)),\n        ExtToTensor(),\n        ExtNormalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225]),\n    ])\n\n    train_dst = CustomData(root_image=imageFolders,root_target=targetFolders,\n                           split='training', transform=train_transform)\n    val_dst = CustomData(root_image=imageFolders,root_target=targetFolders,\n                         split='validation', transform=val_transform)\n    train_loader = data.DataLoader(train_dst, batch_size=batchSize, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader = data.DataLoader(val_dst, batch_size=valBatchSize, shuffle=True, num_workers=4)\n    return train_loader, val_loader\n\n\ndef GetPretrainedModel(device,model,num_classes=17, savedPath=\"/\", pretrained=False,  continue_training=False, optimizer=None, scheduler=None):\n    \"\"\"\n        If pretrained is True\n            Load Weights of Model, Optimizer, Scheduler, CurrentEpoch and BestScore from .pth file provided in savedPath\n            Load Loss and Score for continues training process\n        If pretraind is False\n            Load weights for trasfer learning(city scape weights)\n            Change the output layer with require classes\n        \n    \"\"\"\n    cur_epoch=0\n    best_score=0\n    if pretrained:\n        ckpt=saved_path+\"last_model.pth\"\n        checkpoint = torch.load(ckpt, map_location=torch.device('cpu'))\n        model.load_state_dict(checkpoint[\"model_state\"])\n\n        model.to(device)\n        with open(saved_path+\"Loss.txt\",\"rb\") as File:\n            Loss = pickle.load(File)\n        with open(saved_path+\"Score.txt\",\"rb\") as File:\n            Score = pickle.load(File)\n        if continue_training:\n            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n            cur_epoch = checkpoint[\"epoch\"]\n            best_score = checkpoint['best_score']\n            print(\"Training state restored from %s\" % saved_path)\n        print(\"Model restored from %s\" % saved_path)\n        del checkpoint  # free memory\n    else:\n        model.to(device)\n\n    if continue_training:\n        return cur_epoch, best_score, optimizer, scheduler, model\n    return cur_epoch, best_score, model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Validation Fucntion\ndef validate1(model,loader,metrics,device):\n    \"\"\"\n        validation function for calculating mIoU and IoU for each class using Matric\n    \"\"\"\n    metrics.reset()\n    #validation_loss = []\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(loader):\n            images = images.to(device, dtype=torch.float32)\n            labels = labels.to(device, dtype=torch.long)\n\n            outputs = model(images)\n            preds = outputs.detach().max(dim=1)[1].cpu().numpy()\n            targets=labels.cpu().numpy()            \n            metrics.update(targets, preds)\n\n    return metrics.get_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def TrainModel(imageFolders,targetFolders,learningRate=0.00001,weightDecay=0.0001,learningRatePolicy='poly', noOfEpochs=27,\n              stepSize=10000, savedPath=\"/\",pretrained=False, batchSize=16,valBatchSize=16, lossFunction=\"focal\", useCuda=True):\n  \n    \"\"\"\n        Geting train_loader and val_loader\n        Setting LossFunction, Optimizer and Scheduler\n        Loading weights and varible if pretrained is true\n        Training Loop: loading batch of images, pass to model, weights updation\n        Validation Function to check results\n        Saving Model and Other Required Files to display results\n    \n    \"\"\"    \n    \n    continue_training=True\n    num_classes=17\n    \n    train_loader, val_loader=get_dataset(imageFolders,targetFolders,batchSize,valBatchSize)\n    device = torch.device('cuda' if (torch.cuda.is_available() and useCuda==True) else 'cpu')\n    \n    model = get_efficientunet_b0(out_channels=num_classes, concat_input=True, pretrained=True)\n    \n    \n    #Optimizer, Metric, scheduler and Loss Function\n    \n    optimizer = torch.optim.SGD(model.parameters(), lr=learningRate, momentum=0.9, weight_decay=weightDecay)\n    metrics = StreamSegMetrics(num_classes)\n    \n    if lossFunction==\"focal\":\n        criterion = FocalLoss(ignore_index=255, size_average=True)\n    else:\n        criterion = nn.CrossEntropyLoss(ignore_index=255, reduction='mean')\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=stepSize, gamma=0.1)\n    \n    ##pretraining Code\n    cur_epoch=0\n    best_score=0\n    Loss=[]\n    Score=[]\n    \n    cur_epoch, best_score,optimizer,scheduler, model= GetPretrainedModel(model=model, device=device,num_classes=num_classes, savedPath=savedPath, pretrained=pretrained,  continue_training=continue_training, optimizer=optimizer, scheduler=scheduler)\n    \n    ##Training Loop\n    print(\"Accuracy Of Model at epoch \"+ str(cur_epoch)+ \" is: \"+ str(best_score))\n    \n    interval_loss = 0\n    epochs=cur_epoch+noOfEpochs\n    max_score=10000000000\n    model_curve=[]\n\n    best_score = 0.0\n    cur_itrs=0\n    start = perf_counter()\n    for epoch in tqdm(range(cur_epoch+1,epochs), desc=\"Epochs\"):\n        model.train()\n        running_loss = []\n        for step, (images, labels) in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n            cur_itrs += 1\n            #print(images.shape)\n            #print(labels.shape)\n\n            images = images.to(device, dtype=torch.float32)\n            labels = labels.to(device, dtype=torch.long)\n            optimizer.zero_grad()\n\n            outputs = model(images)\n\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            #end = perf_counter()\n\n            running_loss.append(loss.item())\n            interval_loss += loss.item()\n\n        model.eval()\n        val_score = validate1(model=model, loader=val_loader, metrics=metrics,device=device)\n        print(\"Training Loss\",np.mean(running_loss),\"Validation Loss\")\n        print(metrics.to_str(val_score))\n\n        #if val_score['Mean IoU'] > best_score:  # save best model\n        best_score = val_score['Mean IoU']\n        mkdir(\"./Files\")\n        \n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n            \"scheduler_state\": scheduler.state_dict(),\n            \"best_score\": best_score,\n        }, './Files/last_model.pth')\n\n        Loss.append(np.mean(running_loss))\n        Score.append(val_score)\n\n        with open(\"./Files/Loss.txt\",\"wb\") as File:\n            pickle.dump(Loss,File)\n        with open(\"./Files/Score.txt\",\"wb\") as File:\n            pickle.dump(Score,File)\n\n        scheduler.step()\n        print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch+1, epochs, np.mean(running_loss)))\n    end = perf_counter()\n    print(\"Time\",end-start)\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Function Call\n\nTo train a function, you must give the path of the folders which contains both training and validation folder in it.\n\n* imageFolders are the path of Images\n* targetFolders are the path of Groundtruth\n* noOfEpochs is the epochs\n* if pretrained=True then you must give savedPath=path for a pretrained weights\n* if GPU is available,use useCuda=True\n* Use batchSize for training batch size\n* Use valBatchSize for validation batch size\n* loss function could be 'focal' or 'entropy'\n* 255 is always the ignore index, which means it will not calculate the loss of 255 label\n* Your ground truth must have labels 0 to 16, and it can have 255 to ignore it"},{"metadata":{"papermill":{"duration":0.058449,"end_time":"2020-12-02T07:13:01.517296","exception":false,"start_time":"2020-12-02T07:13:01.458847","status":"completed"},"tags":[],"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"imageFolder= [\n                  '../input/adek20-screen-parsing/ADEChallengeData2016/images',\n                  '../input/adek20-screen-parsing/ADEChallengeData2016/images',\n                '../input/coco-dataset/Coco Stuff Dataset/images',\n                  '../input/custom-sidewalk/custom_sidewalk_updated/customdataset',\n                 ]\n\n\ntargetFolder=[\n        '../input/adk-coco-filter/ADK_COCO_Filter_Anotation_Updated/adk',\n                  '../input/adk-coco-filter/ADK_COCO_Filter_Anotation_Updated/adk_floor_filter',\n                  '../input/adk-coco-filter/ADK_COCO_Filter_Anotation_Updated/coco',\n                  '../input/custom-sidewalk/custom_sidewalk_updated/customannotations',\n            ]\nsaved_path=\"../input/unetb0results/Files/\"\nTrainModel(imageFolders=imageFolder, targetFolders=targetFolder, savedPath=saved_path, pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction Function Setup"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T12:42:15.445066Z","iopub.status.busy":"2020-12-02T12:42:15.444234Z","iopub.status.idle":"2020-12-02T12:42:15.516928Z","shell.execute_reply":"2020-12-02T12:42:15.517925Z"},"papermill":{"duration":0.181203,"end_time":"2020-12-02T12:42:15.518167","exception":false,"start_time":"2020-12-02T12:42:15.336964","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def PredictImage(input_image, useCuda=True,num_classes=17,pretrained=False, saved_path=\"/\" ):\n    \"\"\"\n        Passing Image to Model and Display its overlay and semantic output\n\n    \"\"\"\n    device = torch.device('cuda' if (torch.cuda.is_available() and useCuda==True) else 'cpu')\n    model = get_efficientunet_b0(out_channels=num_classes, concat_input=True, pretrained=True)\n    cur_epoch, best_score, model= GetPretrainedModel(model=model, device=device,num_classes=num_classes, savedPath=saved_path, pretrained=pretrained,  continue_training=False, optimizer=None, scheduler=None)\n    print(\"Accuracy Of Model at epoch \"+ str(cur_epoch)+ \" is: \"+ str(best_score))\n\n    preprocess = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    input_tensor= preprocess(input_image)\n    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n    \n    if torch.cuda.is_available() and useCuda==True:\n        input_batch = input_batch.to('cuda')\n    model.eval()\n    with torch.no_grad():\n        start = perf_counter()\n        output = model(input_batch)[0]\n        end = perf_counter()\n        print(end-start)\n\n    output_predictions = output.argmax(0)\n    pred = CustomData.decode_target(output_predictions.cpu()).astype(np.uint8)\n    vis_segmentation(input_image, output_predictions.cpu())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction Function Call\n\n* input_image: path of the image to predict\n* pretrained: True if model has pretrained weights\n* saved_path : path of pretrained weigths\n* useCuda: True if GPU is available\n* num_classes: no of classes your model has"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T12:42:15.729961Z","iopub.status.busy":"2020-12-02T12:42:15.729239Z","iopub.status.idle":"2020-12-02T12:42:16.295924Z","shell.execute_reply":"2020-12-02T12:42:16.29649Z"},"papermill":{"duration":0.677306,"end_time":"2020-12-02T12:42:16.296639","exception":false,"start_time":"2020-12-02T12:42:15.619333","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"saved_path=\"../input/unetb0results/Files/\"\npreprocess_input= transforms.Compose([\n    transforms.Resize((480,640)),\n])\ninput_image = Image.open(\"../input/my-dataset/images/12.jpg\")\ninput_image=preprocess_input(input_image)\n\nPredictImage(input_image, pretrained=True,saved_path=saved_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Video Function Call\n\n* video_path: path of the video to predict\n* pretrained: True if model has pretrained weights\n* saved_path : path of pretrained weigths\n* useCuda: True if GPU is available\n* num_classes: no of classes your model has\n* num_frames: frames process from the entire video\n* output_path: path of resultant output (.mp4)"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def PredictVideo(video_path,useCuda=True,num_classes=17,pretrained=False, num_frames = 20000 ,saved_path=\"result\",output_path=\"results\"):\n    \n     \n    \"\"\"\n        Function Used to Change a Video to Semantic Overlay Video\n    \"\"\"\n    \n    device = torch.device('cuda' if (torch.cuda.is_available() and useCuda==True) else 'cpu')\n    model = get_efficientunet_b0(out_channels=num_classes, concat_input=True, pretrained=True)\n    cur_epoch, best_score, model= GetPretrainedModel(model=model, device=device,num_classes=num_classes, savedPath=saved_path, pretrained=pretrained,  continue_training=False, optimizer=None, scheduler=None)\n    print(\"Accuracy Of Model at epoch \"+ str(cur_epoch)+ \" is: \"+ str(best_score))\n    \n    preprocess_input= transforms.Compose([\n    transforms.Resize((480,640)),])\n    \n    preprocess = transforms.Compose([\n        transforms.Resize((480,640)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n    np_buff=[]\n    def run_model(input_image):\n        input_tensor= preprocess(input_image)\n        input_batch = input_tensor.unsqueeze(0) \n        if torch.cuda.is_available():\n            input_batch = input_batch.to('cuda')\n            model.to('cuda')\n        model.eval()\n        with torch.no_grad():\n            output = model(input_batch)[0]\n        output_predictions = output.argmax(0)\n        return output_predictions\n    def vis_segmentation_stream(image, seg_map, index):\n        seg_image = CustomData.decode_target(seg_map.cpu()).astype(np.uint8)\n        seg_image=Image.fromarray(seg_image.astype('uint8'), 'RGB')\n\n        background = image.convert(\"RGBA\")\n        overlay = seg_image.convert(\"RGBA\")\n\n        new_img = Image.blend(background, overlay, 0.7)\n        img=np.array(new_img)\n        np_buff.append(img)\n    def run_visualization_video(frame, index):\n        original_im = Image.fromarray(frame[..., ::-1])\n        original_im=preprocess_input(original_im)\n        seg_map = run_model(original_im)\n        vis_segmentation_stream(original_im, seg_map, index)\n        \n    def convert_to_video():\n        print(\"Video path\", output_path)\n        print(\"Output Length\", len(np_buff))\n        video=cv2.VideoWriter(output_path,cv2.VideoWriter_fourcc(*'DIVX'), 15,(np_buff[0].shape[1],np_buff[0].shape[0]))\n        for i in np_buff:\n            i=i[:,:,:-1]\n            video.write(i[:,:,::-1])\n        video.release()\n        print(\"Your result is saved as \",output_path)\n\n    if not os.path.isfile(video_path): \n        print('downloading the sample video...')\n        video_path = urllib.request.urlretrieve('https://github.com/lexfridman/mit-deep-learning/raw/master/tutorial_driving_scene_segmentation/mit_driveseg_sample.mp4')[0]\n        print('running deeplab on the sample video...')\n        \n    print(\"Opening video \", video_path)\n    video = cv2.VideoCapture(video_path)\n    \n    try:\n        start = perf_counter()\n        for i in range(num_frames):\n            _, frame = video.read()\n            if not _: break\n            if i%15==0:\n                run_visualization_video(frame, i)\n        end = perf_counter()\n        sec=end-start\n        print(sec, \"seconds to process video\")\n        convert_to_video()\n        \n       \n    except KeyboardInterrupt:\n        plt.close()\n        print(\"Stream stopped.\")\n        \n  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_path=\"result1.mp4\"\nvideo_path=\"../input/video-test/xyz.mp4\"\nsaved_path=\"../input/unetb0results/Files/\"\nPredictVideo(saved_path=saved_path,pretrained=True,video_path=video_path,num_frames=2000,output_path=output_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Reference Used in this Entire Notebook\n\n#### Utils File Used to Trasform Both Images and Anotation Together\n* https://github.com/VainF/DeepLabV3Plus-Pytorch/blob/master/network/utils.py\n\n#### UNet Architecture\n* !pip install efficientunet-pytorch\n\n#### Focal Loss Snippit\n* https://github.com/JunMa11/SegLoss/tree/master/losses_pytorch\n\n#### Evaluation Matrix\n* https://github.com/VainF/DeepLabV3Plus-Pytorch/blob/master/metrics/stream_metrics.py"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}