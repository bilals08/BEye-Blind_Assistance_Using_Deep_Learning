{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Including Dependencies"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T07:12:37.078688Z","iopub.status.busy":"2020-12-02T07:12:37.07766Z","iopub.status.idle":"2020-12-02T07:12:38.856058Z","shell.execute_reply":"2020-12-02T07:12:38.854429Z"},"papermill":{"duration":1.844091,"end_time":"2020-12-02T07:12:38.856194","exception":false,"start_time":"2020-12-02T07:12:37.012103","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport os\nimport random\nimport argparse\nimport numpy as np\nfrom torch.utils import data\nfrom time import perf_counter\nimport time\nimport torch\nimport torch.nn as nn\nfrom threading import Thread\nimport IPython\n\nimport json\nimport os\nfrom collections import namedtuple\n\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport numpy as np\n\nfrom PIL import Image\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\nimport pickle\nimport zipfile\nimport torch.nn as nn\nimport torch.nn.functional as Fun\nimport torch\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom torchvision import transforms\nimport requests","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utility Functions"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T07:12:39.108825Z","iopub.status.busy":"2020-12-02T07:12:39.093181Z","iopub.status.idle":"2020-12-02T07:12:40.240467Z","shell.execute_reply":"2020-12-02T07:12:40.239566Z"},"papermill":{"duration":1.220612,"end_time":"2020-12-02T07:12:40.24066","exception":false,"start_time":"2020-12-02T07:12:39.020048","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=0, size_average=True, ignore_index=255):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.ignore_index = ignore_index\n        self.size_average = size_average\n\n    def forward(self, inputs, targets):\n        ce_loss = Fun.cross_entropy(\n            inputs, targets, reduction='none', ignore_index=self.ignore_index)\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n        if self.size_average:\n            return focal_loss.mean()\n        else:\n            return focal_loss.sum()\n\n\nclass _StreamMetrics(object):\n    def __init__(self):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()\n\n    def update(self, gt, pred):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()\n\n    def get_results(self):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()\n\n    def to_str(self, metrics):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()\n\n    def reset(self):\n        \"\"\" Overridden by subclasses \"\"\"\n        raise NotImplementedError()      \n\nclass StreamSegMetrics(_StreamMetrics):\n    \"\"\"\n    Stream Metrics for Semantic Segmentation Task\n    \"\"\"\n    def __init__(self, n_classes):\n        self.n_classes = n_classes\n        self.confusion_matrix = np.zeros((n_classes, n_classes))\n\n    def update(self, label_trues, label_preds):\n        #boolarr=label_trues==255\n        #label_preds[boolarr]=255\n        for lt, lp in zip(label_trues, label_preds):\n            self.confusion_matrix += self._fast_hist( lt.flatten(), lp.flatten() )\n    \n    @staticmethod\n    def to_str(results):\n        string = \"\\n\"\n        for k, v in results.items():\n            if k!=\"Class IoU\":\n                string += \"%s: %f\\n\"%(k, v)\n        \n        #string+='Class IoU:\\n'\n        #for k, v in results['Class IoU'].items():\n        #    string += \"\\tclass %d: %f\\n\"%(k, v)\n        return string\n\n    def _fast_hist(self, label_true, label_pred):\n        mask = (label_true >= 0) & (label_true < self.n_classes)\n        hist = np.bincount(\n            self.n_classes * label_true[mask].astype(int) + label_pred[mask],\n            minlength=self.n_classes ** 2,\n        ).reshape(self.n_classes, self.n_classes)\n        return hist\n\n    def get_results(self):\n        \"\"\"Returns accuracy score evaluation result.\n            - overall accuracy\n            - mean accuracy\n            - mean IU\n            - fwavacc\n        \"\"\"\n        hist = self.confusion_matrix\n        acc = np.diag(hist).sum() / hist.sum()\n        acc_cls = np.diag(hist) / hist.sum(axis=1)\n        acc_cls = np.nanmean(acc_cls)\n        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n        mean_iu = np.nanmean(iu)\n        freq = hist.sum(axis=1) / hist.sum()\n        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n        cls_iu = dict(zip(range(self.n_classes), iu))\n#         cls_ac = dict(zip(range(self.n_classes),np.diag(hist)/(hist.sum(axis=1))))\n\n        return {\n                \"Overall Acc\": acc,\n                \"Mean Acc\": acc_cls,\n                \"FreqW Acc\": fwavacc,\n                \"Mean IoU\": mean_iu,\n                #\"Class Acc\": cls_ac,\n                \"Class IoU\": cls_iu,\n            }\n        \n    def reset(self):\n        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n\nclass AverageMeter(object):\n    \"\"\"Computes average values\"\"\"\n    def __init__(self):\n        self.book = dict()\n\n    def reset_all(self):\n        self.book.clear()\n    \n    def reset(self, id):\n        item = self.book.get(id, None)\n        if item is not None:\n            item[0] = 0\n            item[1] = 0\n\n    def update(self, id, val):\n        record = self.book.get(id, None)\n        if record is None:\n            self.book[id] = [val, 1]\n        else:\n            record[0]+=val\n            record[1]+=1\n\n    def get_results(self, id):\n        record = self.book.get(id, None)\n        assert record is not None\n        return record[0] / record[1]\n\n_pil_interpolation_to_str = {\n    Image.NEAREST: 'PIL.Image.NEAREST',\n    Image.BILINEAR: 'PIL.Image.BILINEAR',\n    Image.BICUBIC: 'PIL.Image.BICUBIC',\n    Image.LANCZOS: 'PIL.Image.LANCZOS',\n    Image.HAMMING: 'PIL.Image.HAMMING',\n    Image.BOX: 'PIL.Image.BOX',\n}\nclass ExtResize(object):\n    \"\"\"Resize the input PIL Image to the given size.\n    Args:\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), output size will be matched to this. If size is an int,\n            smaller edge of the image will be matched to this number.\n            i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    \"\"\"\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        #assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img, lbl):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be scaled.\n        Returns:\n            PIL Image: Rescaled image.\n        \"\"\"\n        return F.resize(img, self.size, self.interpolation), F.resize(lbl, self.size, Image.NEAREST)\n\n    def __repr__(self):\n        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str) ","execution_count":2,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T07:12:50.222963Z","iopub.status.busy":"2020-12-02T07:12:50.221774Z","iopub.status.idle":"2020-12-02T07:12:50.225487Z","shell.execute_reply":"2020-12-02T07:12:50.224802Z"},"papermill":{"duration":0.07713,"end_time":"2020-12-02T07:12:50.225695","exception":false,"start_time":"2020-12-02T07:12:50.148565","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from torchvision.transforms.functional import normalize\nimport torch.nn as nn\nimport numpy as np\nimport os \n\ndef denormalize(tensor, mean, std):\n    mean = np.array(mean)\n    std = np.array(std)\n\n    _mean = -mean/std\n    _std = 1/std\n    return normalize(tensor, _mean, _std)\n\nclass Denormalize(object):\n    def __init__(self, mean, std):\n        mean = np.array(mean)\n        std = np.array(std)\n        self._mean = -mean/std\n        self._std = 1/std\n\n    def __call__(self, tensor):\n        if isinstance(tensor, np.ndarray):\n            return (tensor - self._mean.reshape(-1,1,1)) / self._std.reshape(-1,1,1)\n        return normalize(tensor, self._mean, self._std)\n\ndef set_bn_momentum(model, momentum=0.1):\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.momentum = momentum\n\ndef fix_bn(model):\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.eval()\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.mkdir(path)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading DeeplabV3+ Model and MobileNet Backbone"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T07:12:40.793925Z","iopub.status.busy":"2020-12-02T07:12:40.793119Z","iopub.status.idle":"2020-12-02T07:12:40.797412Z","shell.execute_reply":"2020-12-02T07:12:40.796685Z"},"papermill":{"duration":0.096536,"end_time":"2020-12-02T07:12:40.797526","exception":false,"start_time":"2020-12-02T07:12:40.70099","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/input/script/\")\n!python utils.py\n!python mobile_net.py\n!python deeplab.py\n\nfrom utils import *\nfrom mobile_net import *\nfrom deeplab import DeepLabHeadV3Plus\nfrom deeplab import IntermediateLayerGetter\nfrom deeplab import DeepLabV3\n\n!ls\nos.chdir(\"..\")\nos.chdir('../working')\n","execution_count":4,"outputs":[{"output_type":"stream","text":"deeplab.py  mobile_net.py  resnet.py  utils.py\r\n","name":"stdout"}]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T07:12:49.801257Z","iopub.status.busy":"2020-12-02T07:12:49.800156Z","iopub.status.idle":"2020-12-02T07:12:49.802825Z","shell.execute_reply":"2020-12-02T07:12:49.803444Z"},"papermill":{"duration":0.078276,"end_time":"2020-12-02T07:12:49.80359","exception":false,"start_time":"2020-12-02T07:12:49.725314","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#Model Starts from this layer\n\ndef _segm_mobilenet(name, backbone_name, num_classes, output_stride, pretrained_backbone):\n    if output_stride==8:\n        aspp_dilate = [12, 24, 36]\n    else:\n        aspp_dilate = [6, 12, 18]\n\n    backbone = mobilenet_v2(pretrained=pretrained_backbone, output_stride=output_stride)\n    \n    # rename layers\n    backbone.low_level_features = backbone.features[0:4]\n    backbone.high_level_features = backbone.features[4:-1]\n    backbone.features = None\n    backbone.classifier = None\n\n    inplanes = 320\n    low_level_planes = 24\n    \n    if name=='deeplabv3plus':\n        return_layers = {'high_level_features': 'out', 'low_level_features': 'low_level'}\n        classifier = DeepLabHeadV3Plus(inplanes, low_level_planes, num_classes, aspp_dilate)\n    elif name=='deeplabv3':\n        return_layers = {'high_level_features': 'out'}\n        classifier = DeepLabHead(inplanes , num_classes, aspp_dilate)\n    backbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\n\n    model = DeepLabV3(backbone, classifier)\n    return model\n\ndef _load_model(arch_type, backbone, num_classes, output_stride, pretrained_backbone):\n\n    if backbone=='mobilenetv2':\n        model = _segm_mobilenet(arch_type, backbone, num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n    else:\n        raise NotImplementedError\n    return model\n\ndef deeplabv3_mobilenet(num_classes=21, output_stride=8, pretrained_backbone=True, **kwargs):\n    \"\"\"Constructs a DeepLabV3 model with a MobileNetv2 backbone.\n    Args:\n        num_classes (int): number of classes.\n        output_stride (int): output stride for deeplab.\n        pretrained_backbone (bool): If True, use the pretrained backbone.\n    \"\"\"\n    return _load_model('deeplabv3', 'mobilenetv2', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n\ndef deeplabv3plus_mobilenet(num_classes=21, output_stride=8, pretrained_backbone=True):\n    \"\"\"Constructs a DeepLabV3+ model with a MobileNetv2 backbone.\n    Args:\n        num_classes (int): number of classes.\n        output_stride (int): output stride for deeplab.\n        pretrained_backbone (bool): If True, use the pretrained backbone.\n    \"\"\"\n    return _load_model('deeplabv3plus', 'mobilenetv2', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DataLoader"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T07:12:50.063844Z","iopub.status.busy":"2020-12-02T07:12:50.037805Z","iopub.status.idle":"2020-12-02T07:12:50.092082Z","shell.execute_reply":"2020-12-02T07:12:50.091439Z"},"papermill":{"duration":0.119755,"end_time":"2020-12-02T07:12:50.092219","exception":false,"start_time":"2020-12-02T07:12:49.972464","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class CustomData(data.Dataset):\n\n    # Based on https://github.com/mcordts/cityscapesScripts\n    colorMap={\n        \"backgroud\": (225,229 , 204),\n        'wall':(152, 152, 79),\n        'building':(70, 70, 70),\n        'sky':(70, 130, 180),\n        'sidewalk':(244, 35, 232),\n        'field/grass':(152, 251, 152),\n        'vegitation':(107, 142, 35),\n        'person': (220, 20, 60),\n        'mountain':(139, 218, 51),\n        'stairs':(202, 251, 254),\n        'bench':(108, 246, 107),\n        'pole':(153, 153, 153),\n        'car':(41, 34, 177),\n        'bike':(111, 34, 177),\n        'animal':(211, 205, 33),\n        'ground':(147, 147, 136),\n        'fence':(241, 170, 17),\n        'water':(29, 231, 229),\n        'road':(35, 18, 16),\n        'sign_board':(113, 97, 41),\n        'floor':(73, 23, 77),\n        'traffic_light':(225, 175, 57),\n        'ceeling':(51, 0, 0),\n        'unlabelled':(0,0,0),\n        \n    }\n     \n    # Based on https://github.com/mcordts/cityscapesScripts\n    CustomDataSet = namedtuple('CustomDataSet', ['name', 'id', 'train_id', 'category', 'category_id',\n                                                     'has_instances', 'ignore_in_eval', 'color'])\n    classes = [\n            CustomDataSet('backgroud',       0, 0, 'obstacle', 0, False, True, colorMap['backgroud']),\n            CustomDataSet('wall',            1, 1, 'solid', 0, False, True, colorMap['wall']),\n            CustomDataSet('building',        2, 2, 'solid', 0, False, True, colorMap['building']),\n            CustomDataSet('sky',             3, 3, 'backgroud', 0, False, True, colorMap['sky']),\n            CustomDataSet('sidewalk',        4, 4, 'nature', 0, False, True, colorMap['sidewalk']),\n            CustomDataSet('field/grass',     5, 5, 'nature', 0, False, True, colorMap['field/grass']),\n            CustomDataSet('vegitation',      6, 6, 'nature', 0, False, True, colorMap['vegitation']),\n            CustomDataSet('person',          7, 7, 'human', 0, False, True, colorMap['person']),\n            CustomDataSet('mountain',        8, 8, 'nature', 0, False, True, colorMap['mountain']),\n            CustomDataSet('stairs',          9, 255, 'solid', 0, False, True, colorMap['stairs']),\n            CustomDataSet('bench',           10, 0, 'obstacle', 0, False, True, colorMap['bench']),\n            CustomDataSet('pole',            11, 0, 'obstacle', 0, False, True, colorMap['pole']),\n            CustomDataSet('car',             12, 9, 'vahicle', 0, False, True, colorMap['car']),\n            CustomDataSet('bike',            13, 10, 'vahicle', 0, False, True, colorMap['bike']),\n            CustomDataSet('animal',          14, 11, 'animal', 0, False, True, colorMap['animal']),\n            CustomDataSet('ground',          15, 12, 'land', 0, False, True, colorMap['ground']),\n            CustomDataSet('fence',           16, 13, 'solid', 0, False, True, colorMap['fence']),\n            CustomDataSet('water',           17, 14, 'land', 0, False, True, colorMap['water']),\n            CustomDataSet('road',            18, 15, 'land', 0, False, True, colorMap['road']),\n            CustomDataSet('sign_board',      19, 0, 'obstacle', 0, False, True, colorMap['sign_board']),\n            CustomDataSet('floor',           20, 4, 'land', 0, False, True, colorMap['floor']),\n            CustomDataSet('traffic_light',   21, 0,'obstacle', 0, False, True, colorMap['traffic_light']),\n            CustomDataSet('ceeling',         22, 16, 'ceeling', 0, False, True, colorMap['ceeling']),\n            CustomDataSet('unlabelled',      23, 255, 'void', 0, False, True, colorMap['unlabelled']),\n            \n    ]\n    \n    train_id_to_color = [c.color for c in classes if (c.train_id != -1 and c.train_id != 255)]\n    id_to_color = [c.color for c in classes]\n    train_id_to_color.append([0, 0, 0])\n    train_id_to_color = np.array(train_id_to_color)\n    train_id_to_label= [c.name for c in classes if (c.train_id != -1 and c.train_id != 255)]\n    train_id_to_label.append(\"unlabeled\")\n    train_id_to_label = np.array(train_id_to_label)\n    id_to_train_id = np.array([c.train_id for c in classes])\n    \n    def __init__(self, root_image, root_target, split='train', mode='fine', target_type='semantic', transform=None):        \n        \n        self.root_image = [os.path.expanduser(i) for i in root_image ]\n        self.root_target = [os.path.expanduser(i) for i in root_target]\n        \n        self.images = []\n        self.targets = []\n        self.transform = transform\n        self.split = split\n        \n        for i in range(len(self.root_image)):\n            self.root_image[i]=os.path.join(self.root_image[i],split)\n            self.root_target[i]=os.path.join(self.root_target[i],split)\n            print(self.root_image[i])\n            print(self.root_target[i])\n\n            lst=os.listdir(self.root_target[i])\n            \n            for img in lst:\n                if img[0]==\"C\":\n                    self.images.append(os.path.join(root_image[i],img[:-4]+'.jpg'))\n                else:\n                    self.images.append(os.path.join(self.root_image[i],img[:-4]+'.jpg'))\n                self.targets.append(os.path.join(self.root_target[i],img))\n        print(len(self.images))\n        print(len(self.targets))\n\n\n    @classmethod\n    def encode_target(cls, target):\n        return cls.id_to_train_id[np.array(target)]\n\n    @classmethod\n    def decode_target(cls, target):\n        target[target == 255] = 0\n        #target = target.astype('uint8') + 1\n        return cls.train_id_to_color[target]\n\n    def __getitem__(self, index):\n        image = Image.open(self.images[index]).convert('RGB')\n        target = Image.open(self.targets[index])\n        \n       \n        if self.transform:\n            image, target = self.transform(image, target)\n        target = self.encode_target(target)\n        #print(image.shape)\n        #print(target.shape)\n        \n        return image, target\n\n    def __len__(self):\n        return len(self.images)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Setup"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T07:12:50.350547Z","iopub.status.busy":"2020-12-02T07:12:50.349402Z","iopub.status.idle":"2020-12-02T07:12:50.366827Z","shell.execute_reply":"2020-12-02T07:12:50.366174Z"},"papermill":{"duration":0.085468,"end_time":"2020-12-02T07:12:50.366948","exception":false,"start_time":"2020-12-02T07:12:50.28148","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from matplotlib import gridspec\n\ndef create_label_colormap():\n    colormap = CustomData.train_id_to_color\n    return colormap\n\n\ndef label_to_color_image(label):\n    if label.ndim != 2:\n        raise ValueError('Expect 2-D input label')\n\n    colormap = create_label_colormap()\n\n    if np.max(label) >= len(colormap):\n        raise ValueError('label value too large.')\n\n    return colormap[label]\n\n\ndef vis_segmentation(image,seg_map):\n    \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n    plt.figure(figsize=(20, 4))\n    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n\n    plt.subplot(grid_spec[0])\n    plt.imshow(image)\n    plt.axis('off')\n    plt.title('input image')\n\n    plt.subplot(grid_spec[1])\n    seg_image =  CustomData.decode_target(seg_map.cpu()).astype(np.uint8)\n    plt.imshow(seg_image)\n    plt.axis('off')\n    plt.title('segmentation map')\n\n    plt.subplot(grid_spec[2])\n    plt.imshow(image)\n    plt.imshow(seg_image, alpha=0.7)\n    \n    plt.axis('off')\n    plt.title('segmentation overlay')\n\n    unique_labels = np.unique(seg_map)\n    print(\"Uniques Labels Found\",unique_labels)\n    ax = plt.subplot(grid_spec[3])\n    \n    \n    plt.imshow(FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n    ax.yaxis.tick_right()\n    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n    plt.xticks([], [])\n    ax.tick_params(width=0.0)\n    plt.grid('off')\n    plt.show()\n\n\nLABEL_NAMES = CustomData.train_id_to_label\n\nFULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\nFULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Get Datatset\ndef get_dataset(imageFolders,targetFolders,batchSize,valBatchSize):\n    \"\"\" Dataset And Augmentation\n    \"\"\"\n    train_transform = ExtCompose([\n        ExtResize((400,400)),\n        ExtColorJitter( brightness=0.5, contrast=0.5, saturation=0.5 ),\n        ExtRandomHorizontalFlip(),\n        ExtToTensor(),\n        ExtNormalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225]),\n    ])\n    val_transform = ExtCompose([\n        ExtResize((400,400)),\n        ExtToTensor(),\n        ExtNormalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225]),\n    ])\n\n    train_dst = CustomData(root_image=imageFolders,root_target=targetFolders,\n                           split='training', transform=train_transform)\n    val_dst = CustomData(root_image=imageFolders,root_target=targetFolders,\n                         split='validation', transform=val_transform)\n    train_loader = data.DataLoader(train_dst, batch_size=batchSize, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader = data.DataLoader(val_dst, batch_size=valBatchSize, shuffle=True, num_workers=4)\n    return train_loader, val_loader\n\n\n##Validation Fucntion\ndef validate1(model,loader,metrics,device):\n    \n    metrics.reset()\n    #validation_loss = []\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(loader):\n            images = images.to(device, dtype=torch.float32)\n            labels = labels.to(device, dtype=torch.long)\n\n            outputs = model(images)\n            preds = outputs.detach().max(dim=1)[1].cpu().numpy()\n            targets=labels.cpu().numpy()            \n            metrics.update(targets, preds)\n\n    return metrics.get_results()\n\ndef GetPretrainedModel(device,model,num_classes=17, savedPath=\"/\", pretrained=False,  continue_training=False, optimizer=None, scheduler=None):\n    cur_epoch=0\n    best_score=0\n    if pretrained:\n        model.classifier.classifier[3]=nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1)) \n        ckpt=savedPath+\"last_model.pth\"\n        checkpoint = torch.load(ckpt, map_location=torch.device('cpu'))\n        model.load_state_dict(checkpoint[\"model_state\"])\n        model.to(device)\n        with open(savedPath+\"Loss.txt\",\"rb\") as File:\n            Loss = pickle.load(File)\n        with open(savedPath+\"Score.txt\",\"rb\") as File:\n            Score = pickle.load(File)\n\n        cur_epoch = checkpoint[\"epoch\"]\n        best_score = checkpoint['best_score']\n        if continue_training:\n            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n            scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n            print(\"Training state restored from %s\" % savedPath)\n        print(\"Model restored from %s\" % savedPath)\n        del checkpoint  # free memory\n    else:\n        checkpoint=torch.load('../input/cityscape-mobilenet/best_deeplabv3plus_mobilenet_cityscapes_os16.pth',map_location=torch.device('cpu'))\n        model.load_state_dict(checkpoint[\"model_state\"])\n        model.classifier.classifier[3]=nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))    \n        model.to(device)\n\n    return cur_epoch, best_score, model","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def TrainModel(imageFolders,targetFolders,learningRate=0.00001,weightDecay=0.0001,learningRatePolicy='poly', noOfEpochs=27,\n              stepSize=10000, savedPath=\"/\",pretrained=False, batchSize=16,valBatchSize=16, lossFunction=\"focal\", useCuda=True):\n    \n    continue_training=True\n    num_classes=17\n    \n    train_loader, val_loader=get_dataset(imageFolders,targetFolders,batchSize,valBatchSize)\n    device = torch.device('cuda' if (torch.cuda.is_available() and useCuda==True) else 'cpu')\n    \n    model = deeplabv3plus_mobilenet(num_classes=19, output_stride=8)\n    \n    \n    #Optimizer, Metric, scheduler and Loss Function\n    optimizer = torch.optim.SGD(params=[\n                {'params': model.backbone.parameters(), 'lr': 0.1*learningRate},\n                {'params': model.classifier.parameters(), 'lr': learningRate},\n            ], lr=learningRate, momentum=0.9, weight_decay=weightDecay)\n    metrics = StreamSegMetrics(num_classes)\n    \n    if lossFunction==\"focal\":\n        criterion = FocalLoss(ignore_index=255, size_average=True)\n    else:\n        criterion = nn.CrossEntropyLoss(ignore_index=255, reduction='mean')\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=stepSize, gamma=0.1)\n    \n    ##pretraining Code\n    cur_epoch=0\n    best_score=0\n    Loss=[]\n    Score=[]\n    \n    cur_epoch, best_score, model= GetPretrainedModel(model=model, device=device,num_classes=num_classes, savedPath=savedPath, pretrained=pretrained,  continue_training=continue_training, optimizer=optimizer, scheduler=scheduler)\n    \n    ##Training Loop\n    print(\"Accuracy Of Model at epoch \"+ str(cur_epoch)+ \" is: \"+ str(best_score))\n    \n    interval_loss = 0\n    epochs=cur_epoch+noOfEpochs\n    max_score=10000000000\n    model_curve=[]\n\n    best_score = 0.0\n    cur_itrs=0\n    start = perf_counter()\n    for epoch in tqdm(range(cur_epoch+1,epochs), desc=\"Epochs\"):\n        model.train()\n        running_loss = []\n        for step, (images, labels) in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n            cur_itrs += 1\n            #print(images.shape)\n            #print(labels.shape)\n\n            images = images.to(device, dtype=torch.float32)\n            labels = labels.to(device, dtype=torch.long)\n            optimizer.zero_grad()\n\n            outputs = model(images)\n\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            #end = perf_counter()\n\n            running_loss.append(loss.item())\n            interval_loss += loss.item()\n\n        model.eval()\n        val_score = validate1(model=model, loader=val_loader, metrics=metrics,device=device)\n        print(\"Training Loss\",np.mean(running_loss),\"Validation Loss\")\n        print(metrics.to_str(val_score))\n\n        #if val_score['Mean IoU'] > best_score:  # save best model\n        best_score = val_score['Mean IoU']\n        mkdir(\"./Files\")\n        \n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n            \"scheduler_state\": scheduler.state_dict(),\n            \"best_score\": best_score,\n        }, './Files/last_model.pth')\n\n        Loss.append(np.mean(running_loss))\n        Score.append(val_score)\n\n        with open(\"./Files/Loss.txt\",\"wb\") as File:\n            pickle.dump(Loss,File)\n        with open(\"./Files/Score.txt\",\"wb\") as File:\n            pickle.dump(Score,File)\n\n        scheduler.step()\n        print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch+1, epochs, np.mean(running_loss)))\n    end = perf_counter()\n    print(\"Time\",end-start)\n    \n    \n    ","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Function Call\n\nTo train a function, you must give the path of the folders which contains both training and validation folder in it.\n\n* imageFolders are the path of Images\n* targetFolders are the path of Groundtruth\n* noOfEpochs is the epochs\n* if pretrained=True then you must give savedPath=path for a pretrained weights\n* if GPU is available,use useCuda=True\n* Use batchSize for training batch size\n* Use valBatchSize for validation batch size\n* loss function could be 'focal' or 'entropy'\n* 255 is always the ignore index, which means it will not calculate the loss of 255 label\n* Your ground truth must have labels 0 to 16, and it can have 255 to ignore it"},{"metadata":{"papermill":{"duration":0.058449,"end_time":"2020-12-02T07:13:01.517296","exception":false,"start_time":"2020-12-02T07:13:01.458847","status":"completed"},"tags":[],"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"imageFolder= [\n#                   '../input/adek20-screen-parsing/ADEChallengeData2016/images',\n#                   '../input/adek20-screen-parsing/ADEChallengeData2016/images',\n#                   '../input/coco-dataset/Coco Stuff Dataset/images',\n                  '../input/custom-sidewalk/custom_sidewalk_updated/customdataset',\n                 ]\n\n\ntargetFolder=[\n#         '../input/adk-coco-filter/ADK_COCO_Filter_Anotation_Updated/adk',\n#                   '../input/adk-coco-filter/ADK_COCO_Filter_Anotation_Updated/adk_floor_filter',\n#                   '../input/adk-coco-filter/ADK_COCO_Filter_Anotation_Updated/coco',\n                  '../input/custom-sidewalk/custom_sidewalk_updated/customannotations',\n            ]\n\nsaved_path=\"../input/version-10/Files/\"\n\nTrainModel(imageFolders=imageFolder, targetFolders=targetFolder, savedPath=saved_path, pretrained=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction Function Setup"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T12:42:15.445066Z","iopub.status.busy":"2020-12-02T12:42:15.444234Z","iopub.status.idle":"2020-12-02T12:42:15.516928Z","shell.execute_reply":"2020-12-02T12:42:15.517925Z"},"papermill":{"duration":0.181203,"end_time":"2020-12-02T12:42:15.518167","exception":false,"start_time":"2020-12-02T12:42:15.336964","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def PredictImage(input_image, useCuda=True,num_classes=17,pretrained=False, saved_path=\"/\" ):\n    \n    device = torch.device('cuda' if (torch.cuda.is_available() and useCuda==True) else 'cpu')\n    model = deeplabv3plus_mobilenet(num_classes=19, output_stride=8)\n    cur_epoch, best_score, model= GetPretrainedModel(model=model, device=device,num_classes=num_classes, savedPath=saved_path, pretrained=pretrained,  continue_training=False, optimizer=None, scheduler=None)\n    print(\"Accuracy Of Model at epoch \"+ str(cur_epoch)+ \" is: \"+ str(best_score))\n\n    preprocess = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    input_tensor= preprocess(input_image)\n    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n    \n    if torch.cuda.is_available() and useCuda==True:\n        input_batch = input_batch.to('cuda')\n    model.eval()\n    with torch.no_grad():\n        start = perf_counter()\n        output = model(input_batch)[0]\n        end = perf_counter()\n        print(end-start)\n\n    output_predictions = output.argmax(0)\n    pred = CustomData.decode_target(output_predictions.cpu()).astype(np.uint8)\n    vis_segmentation(input_image, output_predictions.cpu())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction Function Call\n\n* input_image: path of the image to predict\n* pretrained: True if model has pretrained weights\n* saved_path : path of pretrained weigths\n* useCuda: True if GPU is available\n* num_classes: no of classes your model has"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-02T12:42:15.729961Z","iopub.status.busy":"2020-12-02T12:42:15.729239Z","iopub.status.idle":"2020-12-02T12:42:16.295924Z","shell.execute_reply":"2020-12-02T12:42:16.29649Z"},"papermill":{"duration":0.677306,"end_time":"2020-12-02T12:42:16.296639","exception":false,"start_time":"2020-12-02T12:42:15.619333","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"saved_path=\"../input/version-10/Files/\"\ninput_image = Image.open(\"../input/adek20-screen-parsing/ADEChallengeData2016/images/validation/ADE_val_00001902.jpg\")\n\nPredictImage(input_image, pretrained=True,saved_path=saved_path)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}